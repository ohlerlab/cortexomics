
# the qc step at the moment assumes that the data is single ended
import csv
import glob
from  pathlib import Path
shell.executable("bash")
shell.prefix("set -e  pipefail;")
# user set parameter
TMPDIR = '../tmp'
SCRIPTDIR = '../git/rna_seq/scripts'

def is_nonempty(file):
  assert os.stat(file).st_size
def is_over_size(file,n):
  assert os.stat(file).st_size > n

# #reference genome
REF_orig = 'pipeline/GRCm38.p5.genome.chr_scaff.fa'
# 'wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M12/ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M12/gencode.vM12.annotation.gtf.gz -O ../annotation/gencode.vM12.annotation.gtf.gz; gunzip ../annotation/gencode.vM12.annotation.gtf.gz'
# 'wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M12/gencode.vM12.chr_patch_hapl_scaff.annotation.gff3.gz -O ../annotation/gencode.vM12.chr_patch_hapl_scaff.annotation.gff3.gz; gunzip ../annotation/gencode.vM12.chr_patch_hapl_scaff.annotation.gff3.gz'
GTF_orig = '../annotation/gencode.vM12.annotation.gtf'
GFF_orig = '../annotation/gencode.vM12.annotation.gff3'
SAMPLE_FILE = "sample_parameter.csv"

#STAR uses the rsem index
RSEMINDEXFOLD="rsemref"
STARINDEX = RSEMINDEXFOLD

# used by infer_experiment
REF = 'my_'+os.path.splitext(os.path.split(REF_orig)[1])[0]+'.fa'
ANNOBASE = 'my_'+os.path.splitext(os.path.split(GFF_orig)[1])[0]

GFF = ANNOBASE+'.gff3'
GTF = ANNOBASE+'.gtf'
CDSGTF = ANNOBASE+'.cdsfilt.gtf'
BED = ANNOBASE+'.bed'
RNAFASTA = ANNOBASE+'.transcript.fa'
CDSFASTA = ANNOBASE+'.cds.fa'

# used by qc
KINDEXCDS = 'kallistoindex/mouse_encode_m12_cds.kdx'
KINDEX    = 'kallistoindex/gencode.vM12.transcripts.kdx'
KMERSIZE  = 21
kallistobin = "~/work/bin/kallisto"

#
filter_index = '../ext_data/contaminants.done'

sample_param = csv.DictReader(open(SAMPLE_FILE))

#sample - info dictionaries
sample_param = list(csv.DictReader(open(SAMPLE_FILE)))
samples = [s['sample_id'] for s in sample_param]
LIBRARY_DICT          = dict((s['sample_id'], s['library_layout']) for s in sample_param)
READ_PATTERN_DICT     = dict((s['sample_id'], s['read_pattern']) for s in sample_param)
PROTOCOL_DICT         = dict((s['sample_id'], s[ 'protocol']) for s in sample_param)
FRAG_LENGTH_MEAN_DICT = dict((s['sample_id'], s[ 'fragment_length_mean']) for s in sample_param)
FRAG_LENGTH_SD_DICT   = dict((s['sample_id'], s[ 'fragment_length_sd']) for s in sample_param)
ASSAY_DICT            = dict((s['sample_id'], s[ 'assay']) for s in sample_param)

GTF_DICT              = {k: CDSGTF if 'ribo' in v else GTF for k, v in ASSAY_DICT.items()}
KINDEXDICT            = {k: KINDEXCDS if 'ribo' in v else KINDEX for k,v in ASSAY_DICT.items()}

#the group dict is structured differently, returns a list of samples
GROUP_DICT = dict((s['sample_id'], s[ 'group']) for s in sample_param)
GROUP_samples = {}

for k, v in GROUP_DICT.items():
    GROUP_samples[v] = GROUP_samples.get(v, [])
    GROUP_samples[v].append(k)

GROUPS = list(GROUP_samples.keys())

#information on the strand of stuff
strands = ['pos','neg']
STRANDSYMS={strands[0]:'+',strands[1]:'-'}

#extensions for transcript and chromosome bigwigs
istransvals = ['.transcript','.chr']
#extensions used by STAR to denot the transcript/genomic bam
BEXTS={istransvals[0]:'.star_transcript',istransvals[1]:''}

RIBO_TOTAL_DICT = dict(zip(
  list(filter(lambda x: 'ribo' in x,samples)),
  list(filter(lambda x: 'total' in x,samples))
))

GENEREGIONS = ['gene','cds','fputrs','tputrs']
# generegions = ['gene','cds','fputrs','tputrs','cds_tiles','fputr_tiles','tputr_tiles']

# TRNAs = ['gencode.vM12.tRNAs.gtf.gz']
TRNAs = ['tRNAs']

# READRANGES = ['25_30','1_26','27_28','29_100','1_300']
READRANGES = ['21_22','25_31','1_300']
# READRANGENUM = [[25,30],[1,26],[27,28],[29,100],[1,300]]
READRANGENUM = [[21,22],[25,31],[1,300]]
READRANGEDICT = dict(zip(READRANGES,READRANGENUM))


clustermethods = ['kmeans','tsne']
clusterdata=[
'cent_scaled_exprdata',
'limma_fold_changes',
# 'composite_fold_changes',
# 'composite_fold_changes_ribodfilt',
'cent_scaled_exprdata_ribodfilt',
'limma_fold_changes_ribodfilt']


#for f in $(echo input/*); do for q in $( echo ${f}/* ); do echo $f $q; done; done | sed 's/input\///' > pipeline/sample_file.txt
SAMPLEFASTQLINES = [line.strip().split(' ') for line in open("sample_file.txt").readlines()]
FASTQS = [l[1] for l in SAMPLEFASTQLINES]
fastqsamples = [l[0] for l in SAMPLEFASTQLINES]
FASTQSAMPLEDICT = dict(zip(FASTQS,fastqsamples))
SAMPLEFASTQDICT = {v:[i for i in FASTQSAMPLEDICT.keys() if FASTQSAMPLEDICT[i] == v ] for k,v in FASTQSAMPLEDICT.items()}


assert set(fastqsamples) == set(samples)

# assert set(ASSAY_DICT.values()) in set(['ribo','total'])

satan_annot_script =  '/fast/groups/ag_ohler/work/dharnet_m/satann_working/Annot_make_bioc_gtf.R'
riboqc_script =  '/fast/groups/ag_ohler/work/dharnet_m/satann_working/analysis_qc_mod_jan2018_12.R'


ribosamples=list(filter(lambda s: ASSAY_DICT[s] == 'ribo',samples))
ribosamples=list(filter(lambda s: not 'test' in s,ribosamples))
totalsamples=list(filter(lambda s: ASSAY_DICT[s] == 'total',samples))
riboms_total_file='/fast/groups/cubi/projects/2017-10-10_cortexomics/gdrive/cortexomics_ms_total/325_new_2_all_log2_LFQ_n7464.txt'
ms_spec_file='/fast/groups/cubi/projects/2017-10-10_cortexomics/gdrive/cortexomics_ms_cyt+80+Poly/proteinGroups.txt'

ribofastqs=list(filter(lambda fq: FASTQSAMPLEDICT[fq] in ribosamples,FASTQS))
totalfastqs=list(filter(lambda fq: FASTQSAMPLEDICT[fq] == totalsamples,FASTQS))

ms_total_file='../gdrive/cortexomics_ms_total/325_new_2_all_log2_LFQ_n7464.txt'
ms_spec_file='../gdrive/cortexomics_ms_cyt+80+Poly/proteinGroups.txt'

nontestribosamples = list(filter(lambda x: not 'test' in x,ribosamples))

# motseqsets = ['translregged','uptranslregged','downtranslregged','xtailtranslregged','xtaildowntranslregged','xtailuptranslregged','randomset']
motseqsets = ['xtailtranslregged','xtaildowntranslregged','xtailuptranslregged','randomset']
# motseqregions = ['CDS','three_prime_UTR','five_prime_UTR']
#ame/cds/techangegenes/.done
motseqregions = [
#'tr',
#'cds',
'fputr',
'tputr'
]
motseqsets = [ 
  # 'techangegenes',
  # 'teupgenes',
  # 'tedowngenes',
  # 'extrtechangegenes',
  # 'extrteupgenes',
  # 'extrtedowngenes',
  'teupgeneslmatch',
  'tedowngeneslmatch'
  # 'teupgenesshuff',
  # 'tedowngenesshuff'
]



samplegroups = {
  'New': list(filter(lambda s: 'RPI' in s,ribosamples)),
  'old': list(filter(lambda s: not 'RPI' in s,ribosamples)),
}
groupnames = list(samplegroups.keys())
samplegroups.update(dict(zip(samples,[[s] for s in samples] )))


#
MAPPABILITY_LENGTHS = [16,20,27]

ALIGNER_TO_USE='star'


#this configures whether we want to trim ids from e.g. ENSG0000001.1 to ENSG000001
# if config['TRIM_IDS']: 
mod_id_sed_cmd = r''' sed -r 's/((transcript_id|gene_id|protein_id|ID|Parent|exon_id|havana_gene|havana_transcript)\W+\w+)\.[0-9]+/\1/g' '''
# else:
  # mod_id_sed_cmd = ' cat '

mainribosamples=list(filter(lambda s:  'ribo' in s,samples))

rule all:
  input:
    FASTQS,
    expand('ame/{reg}/{set}/.done',reg=motseqregions,set=motseqsets),
    expand('deepshapeprime/{sample}/runlog_199.txt',sample=samplegroups['old']),
    expand('salmon/data/{sample}/.done',sample=list(filter(lambda s:  'total' in s,samples))),

MINREADLENGTH=12
MAXREADLENGTH=300
QUALLIM=20
CUTADAPTBIN="~/work/bin/cutadapt"
REMOVE8NBIN="~/work/bin/remove8N_twoarg.pl"
ribowc = '.*(ribo|_Poly|_80S|test|mappability).*'


rule link_in_ref:
  input: REF_orig
  output: REF
  shell:r"""
      ln -fs {REF_orig} {REF}
      """

rule link_in_files:
  input: 'input/{sample}/{fastq}'
  output: 'preprocessed_reads/{sample}/{fastq}'
  run:  
    sample = wildcards['sample']
    fastq = wildcards['fastq']
    shell(r"""
      mkdir -p $(dirname {output})
      ln -sf $(readlink -f input/{sample}/{fastq}) {output}
    """)
    is_over_size(output[0],100)


rule cutadapt_reads:
  input: 'preprocessed_reads/{sample}/{fastq}'
  output: 'cutadapt_reads/{sample}/{fastq}'
  run:
    sampMINREADLENGTH = MINREADLENGTH if not '16bp' in wildcards['sample'] else 0 
    sample = wildcards['sample']

    shell(r"""    #   set -evx
       source activate cutadapt
       mkdir -p cutadapt_reads/{sample}/
       
        zcat {input} \
           | cutadapt \
             -a TGGAATTCTCGGGTGCCAAGG \
            --minimum-length {sampMINREADLENGTH} \
            --maximum-length {MAXREADLENGTH} \
            -q {QUALLIM} - \
        2> cutadapt_reads/{sample}/{wildcards.fastq}.cutadaptstats.txt \
        | gzip  > {output}
    """)
    is_over_size(output[0],100)

rule collapse_reads:
    input: 'cutadapt_reads/{sample}/{fastq}'
    output: 'collapse_reads/{sample}/{fastq}'
    run:
        sample = wildcards['sample']
        colreadstatfile = 'collapse_reads/'+wildcards['sample']+'/'+wildcards['fastq']+'.collreadstats.txt'
        shell(r"""
       set -evx
     
       mkdir -p collapse_reads/{sample}/
    
       zcat {input}  \
         | ~/work/bin/collapse_reads.pl {wildcards.sample} \
         2> {colreadstatfile} \
         | cat > {output}
     """)
        is_over_size(output[0],100)
        assert not "out of mem" in '\n'.join(open(colreadstatfile).readlines())
 
#this finds small filse
 # find collapse_reads/ -name "*.fastq.gz" -size -100M
# find trim_reads/ -name "*.fastq.gz" -size -10M  | xargs ls -latr
# #this finds everything in a certain rule that's less than 10M and then quits
# for i in $(find trim_reads/ -name "*.fastq.gz" -size -10M);do   find . -name $(dirname $i | xargs basename) | grep -v input | grep -v cutadapt; done
rule trim_reads:
    input: 'collapse_reads/{sample}/{fastq}'
    output: 'trim_reads/{sample}/{fastq}'
    wildcard_constraints: sample='.*(ribo|_Poly|_80S).*'
    run:
        sample = wildcards['sample']
        shell(r"""
       set -evx
     
       OUTDIR=$(dirname {output})
       mkdir -p  $OUTDIR
     
       {REMOVE8NBIN} {input} {output}

       gzip -f {output}
       mv {output}.gz {output}

     """)



rule make_trna_rrna_indices:
  input: ancient(GTF)
  output: touch('filter_reads/tRNA_rRNA_index/tRNA_rRNA_index.done')
  run:
    outprefix = output[0].replace('.done','')
    fafile =outprefix+'.fa'
    shell(r"""
      #get rRNAs and tRNAs, rename the tRNAs to exons for GenePRed,
      #get the gene type out and stick it front of the transcript id
      #for better names in the fasta
      cp ../ext_data/contaminants.fa {fafile}

       cat  ../ext_data/rRNAseqs/*.fasta >> {fafile} 
       bowtie2-build {fafile} {outprefix} -p {threads}
       
      """
)

rule make_bowtie_indices:
  input: REF,RNAFASTA
  output: touch('bowtie_index/.done')
  threads: 8
  run:
    outprefix = output[0].replace('.done','')
    fafile =outprefix+'.fa'
    shell(r"""
      #get rRNAs and tRNAs, rename the tRNAs to exons for GenePRed,
      #get the gene type out and stick it front of the transcript id
      #for better names in the fasta
       bowtie2-build {REF} bowtie_index/ref 
       cp {REF} bowtie_index/ref.fa
       bowtie2-build {RNAFASTA} bowtie_index/transcriptome
       cp {RNAFASTA} bowtie_index/transcriptome.fa
       
      """)


rule testplugin:
      input: 'input/{sample}/{fastq}'
      output: 'trim_reads/{sample,.*test.*}/{fastq}'
      shell:r""" mkdir -p $(dirname {output}) ; ln -sf $(readlink -f {input}) {output}"""

collate_idxscript = "../exploration/collate_idx.R"

rule filter_tRNA_rRNA:
    input: 
      'trim_reads/{sample}/{fastq}',
      'filter_reads/tRNA_rRNA_index/tRNA_rRNA_index.done'  

      # filter_index    
    output: 'filter_reads/{sample}/{fastq,.*fastq.gz}',
    threads: 8
    conda: '../envs/tophat'
    params:
      indexname = lambda wc,input: input[1].replace('.done',''),
      outdir = lambda wc,output: os.path.dirname(output[0])
    shell: r"""
       set -evx

       [ -f {params.outdir} ] && rm -rf {params.outdir}
     
       mkdir -p  {params.outdir}

      bowtie2 \
        -x {params.indexname} \
        -L 20  \
        -p {threads}  \
        -N 0 \
        -U  {input[0]} \
        --un-gz {output[0]} \
        --no-unal \
        2> {output[0]}.alignreport.log > {output[0]}.filtered_reads.sam


        samtools view -bh  {output[0]}.filtered_reads.sam \
        | samtools sort -@ {threads}  > {output[0]}.filtered_reads.bam

      samtools index {output[0]}.filtered_reads.bam
      
      #those which mismatch twice should not be included
      samtools view -hb {params.outdir}/filtered_reads.bam \
      | bamtools filter -tag XM:2-10 -in - -out /dev/stdout \
      | samtools view -H > {output[0]}.mm.sam
      #>> {params.outdir}/unmapped.sam
     
      #group the idx columns stuff is from 
      samtools idxstats {output[0]}.filtered_reads.bam \
      | perl -lanpe 's/^(\S+)_[^_\s]+\t/$1\t/' > {output[0]}.idxtmp

      #Rscript --vanilla {collate_idxscript} {output[0]}.idxtmp {params.indexname}.fa

      samtools stats {output[0]}.filtered_reads.bam > samtools stats {output[0]}.filtered_reads.bam.stats

    """
        # (?# --al-gz $OUTDIR/rRNA_reads  \)
        # --un-gz $OUTDIR/tmp \
 
# print(SAMPLEFASTQDICT)
#this rule is the 'signal spliter where we go from sample to indiv fastqs
rule link_processed_reads:
  input: 
    lambda wc: [fastq.replace('input/','filter_reads/') for fastq in SAMPLEFASTQDICT[wc['sample']]]
  output: touch('processed_reads/{sample,.*(ribo|_Poly|_80S|test|mappability).*}/.done')
  wildcard_constraints: sample=ribowc
  shell:r"""
        mkdir -p processed_reads/{wildcards.sample}
        ln -rifs $(readlink -f {input}) processed_reads/{wildcards.sample}/
    """

rule link_total_fastq:
  input: 'preprocessed_reads/{sample}/{sample}.fastq.gz'
  output: touch('processed_reads/{sample,.*(total|_Input_).*}/.done')
  run:
    sample = wildcards['sample']
    shell(r"""
      set -evx
      mkdir -p processed_reads/{sample}/
      ln -sf $(readlink -f {input}) processed_reads/{sample}/{sample}.fastq.gz
    # ln -rsf $(readlink -f {input}/*) processed_reads/{sample}/{sample}.fastq.gz
    """)

KMERSIZE=6


chromsizes = REF.replace('.fa','.chromsizes')
trsizes = GTF.replace('.gtf','.trsizes')

rule makeGTF:
  input: GTF=GTF_orig
  output: GTF
  conda: '../envs/gffread'
  #conda: '~/miniconda3/envs/seq/bin/gffread'
  shell: r""" 
      # set -x
      #with filtering output all sequences
      zless {input.GTF} \
      | {mod_id_sed_cmd} \
      | gffread -F -T -o {GTF}

    """

 
rule make_utrs:
  input: GFF=GFF
  output: fputrs='fputrs.gtf',tputrs='tputrs.gtf'
  # script: 'make_utrfiles.R'
  run:
    shell(r"""
      set -ex
      # module load Cufflinks/2.2.1
      #with filtering output all squences
      cat {input.GFF}  \
      | awk -v OFS="\t"  '{{if($3=="five_prime_UTR"){{         ;print $0}}}}' \
      | sed -r  's/((transcript_id|gene_id|protein_id|ID=\w+|Parent)\W+\w+)\.[0-9]+/\1/g' \
      | gffread - -T -o {output.fputrs} 

      cat {input.GFF} \
      | awk -v OFS="\t"  '{{if($3=="three_prime_UTR"){{         ;print $0}}}}' \
      | sed -r  's/((transcript_id|gene_id|protein_id|ID=\w+|Parent)\W+\w+)\.[0-9]+/\1/g' \
      | gffread - -T -o {output.tputrs}

     
      """) 

rule fastqc:
     input: 'processed_reads/{sample}/.done'
     output: touch('fastqc/data/{sample}/.done')
     # conda: 'fastqc'
     threads: 4
     log:'fastqc/reports/{sample}/fastqc.log'
     params:
      reads = lambda wc: [fq.replace('input/','processed_reads/') for fq in SAMPLEFASTQDICT[wc['sample']]],
      outdir = lambda wc: 'fastqc/data/'+wc.sample+'/'
     shell: '''
          OUTDIR=$(dirname {output[0]})
          mkdir -p {params.outdir}
          wait $(for i in {params.reads}; do $( fastqc -o {params.outdir} $i ) & done) 
        '''

def get_fastqops(inputdir,read_pattern,lstring='<( zcat ',rstring=')'):
  import glob as glob
  #get our input files, in either paired end or single end form
  assert '-f' in read_pattern
  fastqops = read_pattern.split('-')[1].replace('f ','',1).strip()
  fastqops = glob.glob(inputdir+'/'+fastqops)
  fastqops.sort()
  assert fastqops
  assert all([os.stat(fastq).st_size!=0 for fastq in fastqops])
  fastqops = ' '.join(fastqops)
  
  fastqops = lstring+fastqops+')'

  if '-q' in read_pattern:
    fastqops2 = read_pattern.split('-')[2].replace('q ','',1).strip()
    fastqops2 = glob.glob(inputdir+'/'+fastqops2)
    assert all([os.stat(fastq).st_size!=0 for fastq in fastqops2])
    fastqops2.sort()
    fastqops2 = ' '.join(fastqops2)
    fastqops2 = lstring+fastqops2+')'
    fastqops += ' ' + fastqops2
  return(fastqops)

rule star:
     input:
          fastqs='processed_reads/{sample}/.done',
          STARINDEX=STARINDEX,
          bowtie_index='bowtie_index/.done',
     output:
          done = touch('star/data/{sample,[^/]+}/.done')
     threads: 8
     run:
          input.STARINDEX=input.STARINDEX.replace('.done','')
          markdup = '' if ASSAY_DICT[wildcards['sample']] == 'ribo' else '-m'
          platform = 'NotSpecified'
          inputdir = os.path.dirname(input['fastqs'])
          outputdir = os.path.dirname(output[0])
          read_pattern = READ_PATTERN_DICT[wildcards['sample']]
          fastqops = get_fastqops(inputdir,read_pattern,lstring='<( zcat ',rstring=')')
          repdir = outputdir.replace('data','reports')
          tophatindex =input['bowtie_index'].replace('.done','')
          
          halfthreads = threads/2
          sortmem = str(int(5000/halfthreads))+'M'

          remap = '1' if ASSAY_DICT[wildcards['sample']] == 'ribo' else ''
          remap = ''
          
          sample = wildcards['sample']
          
          outsamrgline =  " \"ID:%s\" \"SM:%s\" \"PL:%s\" " % (sample,sample,platform)
          
          shell(r"""
            set -x
         MY_TMP_DIR=$(mktemp -d)
         #MY_TMP_DIR=/fast/users/dharnet_m/tmp/test
        trap "set -x; rm -rf ${{MY_TMP_DIR}}" EXIT KILL TERM INT HUP

         mkdir -p $MY_TMP_DIR
        mkdir -p $MY_TMP_DIR/star
        mkdir -p $MY_TMP_DIR/tophat2


        
        STAR \
              --genomeDir {input.STARINDEX} \
              --runThreadN {threads} \
              --outSAMunmapped Within \
              --outFilterType BySJout \
              --outMultimapperOrder Random \
              --alignSJoverhangMin 8 \
              --alignSJDBoverhangMin 1 \
              --outFilterMismatchNmax 999 \
              --outFilterMismatchNoverLmax 0.04 \
              --alignIntronMin 20 \
              --alignIntronMax 1000000 \
              --alignMatesGapMax 1000000 \
              --genomeLoad NoSharedMemory \
              --quantMode GeneCounts \
              --outSAMattributes NH HI AS NM MD \
              --outSAMtype BAM  Unsorted\
              --outSAMattrRGline {outsamrgline}\
              --outFileNamePrefix ${{MY_TMP_DIR}}/star/ \
              --outReadsUnmapped Fastx \
              --readFilesIn {fastqops}

          iftophat=
          if [ {remap} && ${{MY_TMP_DIR}}/star/Unmapped.out.mate* ]; then
            iftophat=true
            source activate tophat
            tophat2 \
                -p {threads} \
                -z0 \
                -g 100 \
                --output-dir ${{MY_TMP_DIR}}/tophat2 \
                --library-type fr-unstranded \
                --no-coverage-search \
                --transcriptome-index {tophatindex}/transcriptome \
                {tophatindex}/ref \
                ${{MY_TMP_DIR}}/star/Unmapped.out.mate*

            umapped=${{MY_TMP_DIR}}/tophat2/unmapped.bam
            tmapped=
            
            samtools merge \
              -@ {threads}  -f \
             ${{MY_TMP_DIR}}/all.bam \
             ${{MY_TMP_DIR}}/star/Aligned.out.bam \
             ${{MY_TMP_DIR}}/tophat2/*.bam
          else
            cp ${{MY_TMP_DIR}}/star/Aligned.out.bam ${{MY_TMP_DIR}}/all.bam
          fi
          
         samtools sort \
          -@ {halfthreads}\
          -m {sortmem} \
          -T ${{MY_TMP_DIR}} \
          -o {outputdir}/{sample}.bam \
          ${{MY_TMP_DIR}}/all.bam

        #add the read length as a tag to our bam
        MY_TMP_BAM=$(mktemp)
        samtools view -h {outputdir}/{sample}.bam | awk -vFS='\t' -vOFS='\t' '{{print($0,"Xl:Z:"length($10))}}'  | samtools view -bh > $MY_TMP_BAM
        mv $MY_TMP_BAM {outputdir}/{sample}.bam
      
        samtools index {outputdir}/{sample}.bam

        mkdir -p {repdir}
        samtools stats {outputdir}/{sample}.bam > {repdir}/{sample}.bamstats.txt
        samtools flagstat {outputdir}/{sample}.bam > {repdir}/{sample}.flagstat.log
        samtools idxstats {outputdir}/{sample}.bam > {repdir}/{sample}.idxstats.log
        
        cp  ${{MY_TMP_DIR}}/star/ReadsPerGene.out.tab {outputdir}/ReadsPerGene.out.tab
        cp  ${{MY_TMP_DIR}}/star/{{Log,SJ}}* {repdir}/
        if [ $iftophat ] ;then cp ${{MY_TMP_DIR}}/tophat2/align_summary.txt {repdir} ;fi

          """)
          
     
transcript_gene_map = 'transcript_gene_map.tsv'
gene_transcript_map = 'gene_transcript_map.tsv'

rule make_gene_transcript_map:
  input: GTF
  output: gene_transcript_map,transcript_gene_map
  run:
    shell(r"""
    cat {input} \
      | grep -Pe'\ttranscript\t'  \
      | perl -lane '/transcript_id\W+([\w\.]+)/;$t=$1; $g=/gene_id\W+([\w\.]+)/;$g=$1;print($g,"\t",$t)' \
      | sort | uniq \
      > {gene_transcript_map}

    cat {gene_transcript_map} \
      | awk '{{print $2,$1}}' > {transcript_gene_map}
    """)
    is_nonempty(gene_transcript_map)


rrna_intervals = 'qc/picard_rrna_intervals.txt'
refflat = 'qc/'+ANNOBASE+'.refflat'
testsample = 'test_16bpRNA'#we need a bamheader for testing

riboqcanno='riboqc/'+Path(GTF).name.replace('.gtf','.matchchrs.gtf')+'_Rannot'


rule make_chrnamefile:
 input: GTF
 output: 'chrnames.txt'
 shell:r""" cut -d$'\t' -f1 {input} | uniq | sort | uniq | grep -v '#' > {input}"""




rule get_offset_model:
  input:
    bam = 'star/data/{sample}/{sample}.bam',
    gtf = GTF,
    REF = REF,
  output: 'seqshift_reads/data/{sample}/seqshiftmodel.rds'
  shell:r"""Rscript ../src/R/Psite_offsets/offsets3.R {input} $(dirname {output})"""

rule quant_riboseq:
  input:
    bam = 'star/data/{sample}/{sample}.bam',
    gtf = GTF,
    REF = REF,
    shiftmodel = 'seqshift_reads/data/{sample}/seqshiftmodel.rds',
  threads:10
  output: 'riboseq_quant/data/{sample}/segment_counts_df.tsv'
  shell:r"""Rscript ../src/R/Psite_offsets/segment_counts.R {input} $(dirname {output})"""


RIBOSEQCPACKAGE = '/fast/work/users/dharnet_m/Applications/RiboseQC'
RIBOSEQCPACKAGE = 'RiboseQC'
rule make_riboseqc_anno:
  input : 
    GTF,
    REF,str(REF)+'.fai'
  output: Path(GTF).with_suffix('.matchchrs.gtf_Rannot')
  params:
    annobase = Path(GTF).name.replace('.gtf','').replace('.gz',''),
    gtfmatchchrs = lambda wc,input: input[0].replace('.gtf','.matchchrs.gtf')
  shell:r"""
    set -x
    awk -vOFS="\t" '{{print $1,0,$2}}' {REF}.fai | bedtools intersect -b - -a {GTF} > {params.gtfmatchchrs} 
    mkdir -p $(dirname {output[0]})
    R -e 'library("{RIBOSEQCPACKAGE}"); prepare_annotation_files(annotation_directory=".",gtf_file="{params.gtfmatchchrs}",annotation_name="{params.annobase}",forge_BS=FALSE, genome_seq=FaFile("{REF}"))'
 """

rule run_riboseqc:
   input: Path(GTF).with_suffix('.matchchrs.gtf_Rannot'),bam=ALIGNER_TO_USE+'/data/{sample}/{sample}.bam'
   output: touch('riboseqc/data/{sample}/{par}.done'),'riboseqc/data/{sample}/{par}_for_ORFquant',report='riboseqc/reports/{sample}/{par}_riboseqcreport.html',
   threads: 4
   params:
     # rescue_all_rls = lambda wc: 'rescue_all_rls=TRUE' if 'allrls' in wc.par else '',
     # manual_cutoffs = lambda wc: 'offsets_df=read.table("../ext_data/orfquant_manual_offsets.tsv")' if 'manualcutoffs' in wc.par else '',
     par = lambda wc: (','+','.join(filter(lambda x: x is not None,[
      ('rescue_all_rls=TRUE' if 'allrls' in wc.par else None) , 
      ('offsets_df=read.table("../ext_data/orfquant_manual_offsets_colflip.tsv",head=T)' if 'manualcutoffs' in wc.par else None)
    ]))+',').replace(',,',','),
     annofile = lambda wc,input: input[0].replace('annot.done',Path(GTF_orig).name.replace('.gtf','.matchchrs.gtf_Rannot')),
     outname = lambda wc,output: output[0].replace('.done',''),
   shell:r"""
         set -x
         mkdir -p {params.outname}
         mkdir -p riboseqc/reports/{wildcards.sample}
         echo {params.par}
         R -e 'library("{RIBOSEQCPACKAGE}");RiboseQC_analysis("{params.annofile}" ,bam="{input.bam}" {params.par} dest_names="{params.outname}", genome_seq = "{REF}", report_file="{output.report})'
     """

#####Added pulling ORFquant from the config
ORFquantPACKAGE ='/fast/work/users/dharnet_m/Applications/ORFquant'
ORFquantPACKAGE ='ORFquant'
rule run_ORFquant:
  input : 'riboseqc/data/{sample}/{par}.done',annofile=Path(GTF).with_suffix('.matchchrs.gtf_Rannot')
  output: touch('ORFquant/{sample}/{par}.done')
  params: 
    for_ORFquantfile = lambda wc,input: 'c("'+('","'.join(['riboseqc/data/'+s+'/'+wc['par']+'_for_ORFquant' for s in [wc['sample']] ]))+'")',

    outputdir = lambda wc,output: output[0].replace('.done','')
  threads: 10
  shell:r"""
    set -ex
      mkdir -p {params.outputdir}
      R -e 'library("{RIBOSEQCPACKAGE}");library("{ORFquantPACKAGE}");run_ORFquant(for_ORFquant_file = {params.for_ORFquantfile},annotation_file = "{input.annofile}", n_cores = {threads},prefix="{params.outputdir}")'
      """


ms_total_file='/fast/groups/cubi/projects/2017-10-10_cortexomics/gdrive/cortexomics_ms_total/325_new_2_all_log2_LFQ_n7464.txt'
ms_spec_file='/fast/groups/cubi/projects/2017-10-10_cortexomics/gdrive/cortexomics_ms_cyt+80+Poly/proteinGroups.txt'




rule make_id_table:
  input: GTF
  output: 'ids.txt'
  shell: r"""R -e 'library(tidyverse,quiet=T); library(rtracklayer,quiet=T);import("{input}") %>%mcols%>%as.data.frame%>% select(gene_id,gene_name)%>%distinct%>%write.table("{output}", col.names=TRUE, row.names=FALSE)'"""

rule create_ms_exprtables:
  input: ms_total_file,ms_spec_file
  output: touch('ms_tables/.done')
  shell: r"""Rscript ../src/load_data/create_ms_exprtables.R {input[0]} {input[1]} $(dirname {output})"""

DISPDIFF = 0

rule run_ribodiff:
  input: 'feature_counts/all_feature_counts'
  # conda: '../envs/ribodiff_filt'
  output: touch('ribodiff/.done')
  shell: r"""source activate ribodiff ; Rscript ../src/R/TE_change/run_ribodiff.R {input} {DISPDIFF} $(dirname {output})"""

DISPDIFF = 0

rule run_xtail:
  input: 'feature_counts/all_feature_counts','SaTAnn/uORFs.feature_counts'
  output: touch('xtail/.done')
  shell: r"""Rscript ../src/R/TE_change/run_xtail.R {input} $(dirname {output})"""

MS_MEASURE_TO_USE = 'LFQ'
mstbl = 'ms_tables/ms_'+MS_MEASURE_TO_USE+'_total_ms_tall.tsv'



#needs to be integrate exprdata2 nwo
rule integrate_exprdata:
  input: 'CDS_rna_counts',mstbl,expand('riboseq_quant/data/{sample}/segment_counts_df.tsv',sample=ribosamples)
  output: 'exprdata/transformed_data.txt','exprdata/cent_scaled_exprdata.txt','exprdata/designmatrix.txt','exprdata/allcounts_snorm.tsv'
  shell: r"""mkdir -p exprdata;
  Rscript ../src/R/Load_data/integrate_exprdata.R {input[0]} {input[1]} {output}"""

rule get_limma_fold_changes:
  input: 'exprdata/transformed_data.txt','exprdata/designmatrix.txt'
  output: 'exprdata/limma_fold_changes.txt','exprdata/limma_fold_changes.txtfull.txt'
  shell: r"""Rscript ../src/Modeling/get_limma_fold_changes.R {input[0]} {input[1]} {output[0]}"""

rule perform_clustering:
  input: 'exprdata/{clusterdata}.txt',
  output: touch('clusters/{clusterdata}/{clustermethod}/.done')
  shell: r"""Rscript ../src/Modeling/cluster_{wildcards.clustermethod}.R {input} $(dirname {output}) 10"""

rule comp_ribodiff_limma:
  input: 'ribodiff/.done','exprdata/limma_fold_changes.txtfull.txt'
  output: 'ribodiff_limma_comp/.done'
  shell: r""" ../src/Modeling/comp_ribodiff_limma.R $(dirname {input[0]}) $(dirname {input[1]}) $(dirname {output}) """

rule ribodiff_filt:
  input: 'ribodiff/.done','ids.txt','exprdata/cent_scaled_exprdata.txt','exprdata/limma_fold_changes.txt',
  output: 'exprdata/{clusterdata,.+_ribodfilt.txt}'
  shell: r"""Rscript ../src/R/TE_change/ribodiff_filter.R {input}"""

rule assess_clustering:
  input: 'clusters/{clusterdata}/{clustermethod}/.done'
  output: 'cluster_assessment/{clusterdata}/{clustermethod}/.done'
  shell: r"""Rscript ../src/Modeling/assessclusters.R $(dirname {output}) $(dirname {output})"""

rule reg_seqs:
  input: genome=REF,tputrs='tputrs.gtf',seqs='ribodiff/riboseqres_P0.txt'
  output: 'regulated_sequences.fa'
  shell: r"""
    R -e   'library(magrittr);library(GenomicRanges);library(data.table);myfasta <- Rsamtools::FaFile("{input.genome}") ; myutrs <- rtracklayer::import("{input.tputrs}") ; diffIDs <-  "{input.seqs}"%>% fread %>% dplyr::filter(padj<0.05) %>% .$geneID; myutrs %>% subset(gene_id %in% diffIDs) %>%split(.,.$gene_id) %>% reduce %>% {{GenomicFeatures::extractTranscriptSeqs(x=myfasta,.)}} %>% {{Biostrings::writeXStringSet(., "{output[0]}")}}'
  """


################################################################################
########ISoform specific quant
################################################################################
 #perl -lanpe 's/\|.*$//' gencode.vM12.pc_transcripts.fa   > gencode.vM12.pc_transcripts_ntrim.fa 
dpGTF='/fast/work/groups/ag_ohler/dharnet_m/cortexomics/ext_data/gencode.vM12.annotation.gtf'
# TRFASTA='/fast/work/groups/ag_ohler/dharnet_m/cortexomics/ext_data/gencode.vM12.pc_transcripts_filter.fa'
TRFASTA='../ext_data/gencode.vM12.pc_transcripts.fa'
ALLTRFASTA='../ext_data/gencode.vM12.transcripts.fa'


maintotalsamples = [s for s in totalsamples if '_total_' in s]
rule alltr_salmon:
  input: expand('salmon_alltrs/data/{sample}/.done',sample=maintotalsamples)

rule make_salmon_index:
  threads: 8
  input:  lambda wc: ALLTRFASTA if wc['seqs'] == '_alltrs' else TRFASTA
  output: salmonindex =  touch('salmonindex{seqs}/.done')
  run:
    shell(""" salmon index -p {threads}  -k 21 -t {input} -i $(dirname {output[0]})""")

rule salmon:
  input:
    fastqs='processed_reads/{sample}/.done',
    salmonindex= lambda wc: 'salmonindex_alltrs/.done' if wc['seqs'] == '_alltrs' else  'salmonindex/.done'
  params:
    salmonindex = lambda wc,input: 'salmonindexribo' if wc['sample'] in ribosamples else input.salmonindex.replace('.done',''),
    lib = lambda wc,input: 'SF' if wc['sample'] in ribosamples else 'SR',
    outdir = lambda wc,input: 'salmon'+wc['seqs']
  output:
      done = touch('salmon{seqs}/data/{sample}/.done')
  threads: 4
  shell:r"""
      set -ex
      mkdir -p {params.outdir}/reports/{wildcards.sample}
      mkdir -p {params.outdir}/data/{wildcards.sample}
      salmon quant \
      -p {threads} \
      -l {params.lib} \
      --seqBias \
      -i {params.salmonindex} \
      -r <(zcat processed_reads/{wildcards.sample}/*.fastq.gz ) \
      --output {params.outdir}/data/{wildcards.sample} \
      --validateMappings
"""

#src/deepshapemakeref.sh
rule star_transcript:
  input:
    fastq='processed_reads/{sample}/{sample}.fastq.gz',
    transcriptindexfold='deepshape/StarIndex/transcript',
  output: 
    bam='star_transcript/data/{sample}/{sample}.sort.bam',
    bai='star_transcript/data/{sample}/{sample}.sort.bam.bai',
  params:
    bamnosort=lambda wc,output: output.bam.replace('sort.','')
  shell:r"""

  mkdir -p star_transcript/data/{wildcards.sample}/{wildcards.sample}.fastq.gz

  STAR --runThreadN 15 --genomeDir {input.transcriptindexfold} \
    --readFilesIn <(zcat {input.fastq}) \
    --outFileNamePrefix star_transcript/{wildcards.sample}.transcript_ \
    --outSAMtype BAM Unsorted \
    --outSAMmode NoQS \
    --outSAMattributes NH NM \
    --seedSearchLmax 10 \
    --outFilterMultimapScoreRange 0 \
    --outFilterMultimapNmax 255 \
    --outFilterMismatchNmax 1 \
    --outFilterIntronMotifs RemoveNoncanonical

  mv star_transcript/{wildcards.sample}.transcript_Aligned.out.bam {params.bamnosort}
  samtools sort {params.bamnosort} -o {output.bam}
  samtools index {output.bam}

  """

DPDIR='/fast/work/groups/ag_ohler/dharnet_m/cortexomics/Applications/DeepShape/DeepShape-prime/'
offsets2col='/fast/work/groups/ag_ohler/dharnet_m/cortexomics/ext_data/offsets_manual.2col.tsv'
rule dpprimeformatbam:
  input: bam='star_transcript/data/{sample}/{sample}.sort.bam'
  output: refabs='deepshapebamdata/{sample}.bam.reformat.absolute',ref='deepshapebamdata/{sample}.bam.reformat'
  conda: '../envs/biopython2.7'
  shell:r"""

   samtools view \
     {input.bam} | \
     awk '{{if(length($10)>=25 && length($10)<=36){{print $1"\t"$3"\t"$4"\t"length($10);}}}}' | \
     awk -F"|" '{{printf $1"\t"$2; for(i=8;i<=NF-1;i++){{if($i~/CDS:/) {{gsub("CDS:","",$i); gsub("-","\t",$i); printf "\t"$i;}}}} printf $NF"\n"}}' \
     > {output[1]}

    python {DPDIR}/ParseBam2RiboPos.py \
      {dpGTF} \
      {TRFASTA} \
      {offsets2col} \
      {output.ref} \
      {output.refabs}

  """

#see integrate countdata for where this gets loaded
rule dpprime:
  input: dpbam='deepshapebamdata/{sample}.bam.reformat.absolute',transcriptfasta=TRFASTA,
  output: file='deepshapeprime/{sample}/runlog_199.txt',
  conda: '../envs/biopython2.7'
  threads: 8
  shell:r"""
    mkdir -p $( dirname {output.file} ) 

    python {DPDIR}/DeepShape-prime.py \
    {input.dpbam} \
    {input.transcriptfasta} \
    $(dirname {output.file}) \
    200
  """

rule dpprimecov:
  input: tpms=rules.dpprime.output.file,bamreformat=rules.dpprimeformatbam.output.refabs
  output: distsig='dpprimecov/{sample}/dpprime_dist_ribosig.tsv'
  shell:r"""
    mkdir -p $(dirname {output.distsig})
    set -x
    #"tpms to reads"
    join -1 6 -2 1 <(sort -k 6 {input.bamreformat}) <(cat {input.tpms} | cut -f 1,2 | sort -k 1) | sort -k 2 > {input.bamreformat}.withtpm
    # "sum tpms"
    datamash -t' ' -g 2 sum 8 < {input.bamreformat}.withtpm > {input.bamreformat}.readtpms
    # "distribute read signal"
    join -1 2 -2 1 {input.bamreformat}.withtpm {input.bamreformat}.readtpms | awk '{{print $0,$8/$9}}' | datamash -t' ' -g 2,6 sum 10 > {output.distsig}
    [[ -s {output.distsig} ]] || rm {output.distsig}
"""



#Motif analysis

pwmfolder = '../ext_data/mouse_cisrbp/pwms_all_motifs/'
pwms = list(filter(lambda f: '.meme' in f, list(os.walk(pwmfolder))[0][2]))


def getamectrlstring(wc,input):
  if 'ranked' in wc.set:
    ctrlstring = ''
  elif 'shuff' in wc.set:
    ctrlstring = '--control --shuffle-- --kmer 2 '
  else:
    ctrlstring = '--control '+input.fasta.replace('.fa', '.background.fa')
  return ctrlstring


def getamefasta(wc):
  reg = wc['reg']
  inset = wc['set']
  inset = inset.replace('shuff','')
  return 'motseqs/'+reg+'/'+inset+'/'+inset+'.fa'

def get_cermit_mots(wc):
  motset = ('terev' if 'down' in wc['set'] else 'teranked')
  return 'motseqs/'+wc['reg']+'/'+motset+'/'+motset+'_cermit/cermitmotifs.meme'

#this was run on the max cluster - cermit won't run on the BIH cluster
#for fa in $( find  motseqs/ ! -iname '*top*' ! -iname '*background*' -iname 'teranked.fa' -o -iname 'terev.fa'  ) ; do NSEQS=2500;awk "/^>/ {n++} n>$NSEQS {exit} {print}" $fa > ${fa%.fa}.top$NSEQS.fa ;echo $fa "\n"; python cermit.py  -fasta ${fa%.fa}.top$NSEQS.fa -output ${fa%.fa}_cermit -motifdb Mus_musculus.wpum.dna.meme  -tomtom_norc -strand_specific_analysis=yes  ; done

rule ame:
  input: 
    fasta = getamefasta ,
    pwmfile='../ext_data/Mus_musculus.wpum.rna.meme'
  output: touch('ame/{reg}/{set}/.done'),'ame/{reg}/{set}/all/ame.html'
  conda: '../envs/meme'
  params: 
    # pwmfiles = lambda wc,input:[input.pwmfolder+'/' + f for f in list(os.walk(pwmfolder))[0][2] if '.meme' in f if 'M287' in f],
    cermitmots = get_cermit_mots,
    meme = lambda wc,output: output[0].replace('.done', 'comb.meme'),
    outdir =   lambda wc,output: output[0].replace('.done', ''),
    ctrlstr =  getamectrlstring
  shell: r"""
      rm -rf {params.outdir}
      mkdir -p {params.outdir}
      cat {params.cermitmots} ../ext_data/tops_yamashita_etal_2008/top_pwm.txt.meme | sed -e  's/ACGT/ACGU/' | meme2meme {input.pwmfile} /dev/stdin > {params.meme}
      ame --scoring totalhits --method fisher {params.ctrlstr} --o {params.outdir}/all --sequence {input.fasta} {params.meme}
      ame --scoring totalhits --method fisher --control --shuffle-- --kmer 2 --o {params.outdir}/all_shuff --sequence {input.fasta} {params.meme}
      [[ -s {params.outdir}/all/ame.html ]]  || rm {params.outdir}/all/ame.html
   # done
  """
chrsizes = REF.replace(".fa",".chromsizes")
