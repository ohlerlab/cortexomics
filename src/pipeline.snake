# the qc step at the moment assumes that the data is single ended
import csv
import glob
from  pathlib import Path
shell.executable("bash")
shell.prefix("set -e  pipefail;")
# user set parameter
TMPDIR = '../tmp'
SCRIPTDIR = '../git/rna_seq/scripts'

def is_nonempty(file):
  assert os.stat(file).st_size
def is_over_size(file,n):
  assert os.stat(file).st_size > n

# #reference genome
REF_orig = 'pipeline/GRCm38.p5.genome.chr_scaff.fa'
# 'wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M12/ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M12/gencode.vM12.annotation.gtf.gz -O ../annotation/gencode.vM12.annotation.gtf.gz; gunzip ../annotation/gencode.vM12.annotation.gtf.gz'
# 'wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M12/gencode.vM12.chr_patch_hapl_scaff.annotation.gff3.gz -O ../annotation/gencode.vM12.chr_patch_hapl_scaff.annotation.gff3.gz; gunzip ../annotation/gencode.vM12.chr_patch_hapl_scaff.annotation.gff3.gz'
GTF_orig = '../annotation/gencode.vM12.annotation.gtf'
GFF_orig = '../annotation/gencode.vM12.annotation.gff3'
SAMPLE_FILE = "sample_parameter.csv"

#STAR uses the rsem index
RSEMINDEXFOLD="rsemref"
STARINDEX = RSEMINDEXFOLD

# used by infer_experiment
REF = 'my_'+os.path.splitext(os.path.split(REF_orig)[1])[0]+'.fa'
ANNOBASE = 'my_'+os.path.splitext(os.path.split(GFF_orig)[1])[0]

GFF = ANNOBASE+'.gff3'
GTF = ANNOBASE+'.gtf'
CDSGTF = ANNOBASE+'.cdsfilt.gtf'
BED = ANNOBASE+'.bed'
RNAFASTA = ANNOBASE+'.transcript.fa'
CDSFASTA = ANNOBASE+'.cds.fa'

# used by qc
KINDEXCDS = 'kallistoindex/mouse_encode_m12_cds.kdx'
KINDEX    = 'kallistoindex/gencode.vM12.transcripts.kdx'
KMERSIZE  = 21
kallistobin = "~/work/bin/kallisto"

#
filter_index = '../ext_data/contaminants.done'

sample_param = csv.DictReader(open(SAMPLE_FILE))

#sample - info dictionaries
sample_param = list(csv.DictReader(open(SAMPLE_FILE)))
samples = [s['sample_id'] for s in sample_param]
LIBRARY_DICT          = dict((s['sample_id'], s['library_layout']) for s in sample_param)
READ_PATTERN_DICT     = dict((s['sample_id'], s['read_pattern']) for s in sample_param)
PROTOCOL_DICT         = dict((s['sample_id'], s[ 'protocol']) for s in sample_param)
FRAG_LENGTH_MEAN_DICT = dict((s['sample_id'], s[ 'fragment_length_mean']) for s in sample_param)
FRAG_LENGTH_SD_DICT   = dict((s['sample_id'], s[ 'fragment_length_sd']) for s in sample_param)
ASSAY_DICT            = dict((s['sample_id'], s[ 'assay']) for s in sample_param)

GTF_DICT              = {k: CDSGTF if 'ribo' in v else GTF for k, v in ASSAY_DICT.items()}
KINDEXDICT            = {k: KINDEXCDS if 'ribo' in v else KINDEX for k,v in ASSAY_DICT.items()}

#the group dict is structured differently, returns a list of samples
GROUP_DICT = dict((s['sample_id'], s[ 'group']) for s in sample_param)
GROUP_samples = {}

for k, v in GROUP_DICT.items():
    GROUP_samples[v] = GROUP_samples.get(v, [])
    GROUP_samples[v].append(k)

GROUPS = list(GROUP_samples.keys())

#information on the strand of stuff
strands = ['pos','neg']
STRANDSYMS={strands[0]:'+',strands[1]:'-'}

#extensions for transcript and chromosome bigwigs
istransvals = ['.transcript','.chr']
#extensions used by STAR to denot the transcript/genomic bam
BEXTS={istransvals[0]:'.star_transcript',istransvals[1]:''}

RIBO_TOTAL_DICT = dict(zip(
  list(filter(lambda x: 'ribo' in x,samples)),
  list(filter(lambda x: 'total' in x,samples))
))

GENEREGIONS = ['gene','cds','fputrs','tputrs']
# generegions = ['gene','cds','fputrs','tputrs','cds_tiles','fputr_tiles','tputr_tiles']

# TRNAs = ['gencode.vM12.tRNAs.gtf.gz']
TRNAs = ['tRNAs']

# READRANGES = ['25_30','1_26','27_28','29_100','1_300']
READRANGES = ['21_22','25_31','1_300']
# READRANGENUM = [[25,30],[1,26],[27,28],[29,100],[1,300]]
READRANGENUM = [[21,22],[25,31],[1,300]]
READRANGEDICT = dict(zip(READRANGES,READRANGENUM))


clustermethods = ['kmeans','tsne']
clusterdata=[
'cent_scaled_exprdata',
'limma_fold_changes',
# 'composite_fold_changes',
# 'composite_fold_changes_ribodfilt',
'cent_scaled_exprdata_ribodfilt',
'limma_fold_changes_ribodfilt']


#for f in $(echo input/*); do for q in $( echo ${f}/* ); do echo $f $q; done; done | sed 's/input\///' > pipeline/sample_file.txt
SAMPLEFASTQLINES = [line.strip().split(' ') for line in open("sample_file.txt").readlines()]
FASTQS = [l[1] for l in SAMPLEFASTQLINES]
fastqsamples = [l[0] for l in SAMPLEFASTQLINES]
FASTQSAMPLEDICT = dict(zip(FASTQS,fastqsamples))
SAMPLEFASTQDICT = {v:[i for i in FASTQSAMPLEDICT.keys() if FASTQSAMPLEDICT[i] == v ] for k,v in FASTQSAMPLEDICT.items()}


assert set(fastqsamples) == set(samples)

# assert set(ASSAY_DICT.values()) in set(['ribo','total'])

satan_annot_script =  '/fast/groups/ag_ohler/work/dharnet_m/satann_working/Annot_make_bioc_gtf.R'
riboqc_script =  '/fast/groups/ag_ohler/work/dharnet_m/satann_working/analysis_qc_mod_jan2018_12.R'


ribosamples=list(filter(lambda s: ASSAY_DICT[s] == 'ribo',samples))
ribosamples=list(filter(lambda s: not 'test' in s,ribosamples))
totalsamples=list(filter(lambda s: ASSAY_DICT[s] == 'total',samples))
riboms_total_file='/fast/groups/cubi/projects/2017-10-10_cortexomics/gdrive/cortexomics_ms_total/325_new_2_all_log2_LFQ_n7464.txt'
ms_spec_file='/fast/groups/cubi/projects/2017-10-10_cortexomics/gdrive/cortexomics_ms_cyt+80+Poly/proteinGroups.txt'

ribofastqs=list(filter(lambda fq: FASTQSAMPLEDICT[fq] in ribosamples,FASTQS))
totalfastqs=list(filter(lambda fq: FASTQSAMPLEDICT[fq] == totalsamples,FASTQS))

ms_total_file='../gdrive/cortexomics_ms_total/325_new_2_all_log2_LFQ_n7464.txt'
ms_spec_file='../gdrive/cortexomics_ms_cyt+80+Poly/proteinGroups.txt'

nontestribosamples = list(filter(lambda x: not 'test' in x,ribosamples))

# motseqsets = ['translregged','uptranslregged','downtranslregged','xtailtranslregged','xtaildowntranslregged','xtailuptranslregged','randomset']
motseqsets = ['xtailtranslregged','xtaildowntranslregged','xtailuptranslregged','randomset']
motseqregions = ['CDS','three_prime_UTR','five_prime_UTR']



samplegroups = {
  'New': list(filter(lambda s: 'RPI' in s,ribosamples)),
  'old': list(filter(lambda s: not 'RPI' in s,ribosamples)),
}
groupnames = list(samplegroups.keys())
samplegroups.update(dict(zip(samples,[[s] for s in samples] )))


#
MAPPABILITY_LENGTHS = [16,20,27]

ALIGNER_TO_USE='star'

rule all:
  input:
    FASTQS,
    # ms_total_file,
    # ms_spec_file,
    # [fastq.replace('input/','cutadapt_reads/') for fastq in ribofastqs],
    [fastq.replace('input/','collapse_reads/') for fastq in ribofastqs],
    # [fastq.replace('input/','trim_reads/') for fastq in ribofastqs],
    expand("processed_reads/{sample}/.done", sample = samples),
    # expand("cutsequences/{sample}/cutseqs.txt.gz", sample = samples),
    # GTF_orig,
    # GFF_orig,
    #expand("rsem/data/{sample}/.done", sample = samples),
    expand("fastqc/data/{sample}/.done", sample = samples),
    #    "fastqc/summary/fastqc_summary.tsv",
    expand("star/data/{sample}/.done", sample = samples),
    # expand("tophat2/data/{sample}/.done", sample = ribosamples),
    #expand("infer_experiment/data/{sample}/.done", sample = samples),
    expand("qc/data/{sample}/.done", sample = samples),
    expand("multiqc/multiqc_report.html"),
    
    # # # expand("dupradar/data/{sample}/.done", sample = samples),
    expand('feature_counts_readrange/data/{sample}/{generegions}/{readrange}/feature_counts', sample=RIBO_TOTAL_DICT.keys(), generegions=GENEREGIONS+TRNAs, readrange=READRANGES),
    expand("feature_counts/data/{sample}/feature_counts", sample = nontestribosamples),
    expand("feature_counts/all_feature_counts"),
    # # expand("feature_counts/data/{sample}/{generegions}/{readrange}/.done", sample = samples, generegions = GENEREGIONS+TRNAs, readrange = READRANGES),
    # # # # expand("kallisto/data/{sample}/.done", sample = samples),
    # expand("bigwigs/{group}/{strand}/{istrans}.done",group = samples,strand=strands,istrans=istransvals),
    # expand("mergedbigwigs/{group}/{strand}/{istrans}.done",group = GROUPS,strand=strands,istrans=istransvals),
    # # expand("ribotaper/{sample}/.done",sample=list(RIBO_TOTAL_DICT.keys())),
    # # expand("ribotapermetaplots/{sample}/.done",sample=list(RIBO_TOTAL_DICT.keys())),
    # ('ms_tables/.done'),
    # expand('exprdata/{clusterdata}.txt',clusterdata=clusterdata),
    ('xtail/.done'),
    # expand('sequence_analysis/{clusterdata}/{clustermethod}/.done',clusterdata=clusterdata,clustermethod=clustermethods),
    # expand('cluster_assessment/{clusterdata}/{clustermethod}/.done',clusterdata=clusterdata,clustermethod=clustermethods),
    expand("riboseq_quant/data/{sample}/segment_counts_df.tsv", sample = ribosamples),
    expand("SaTAnn/{sample}/.done", sample = ribosamples),
    # expand('riboqc/reports/{sample}/riboqcreport.html', sample = ribosamples+groupnames),
    # expand('motscans/{reg}/{set}/.done',reg=motseqregions,set=motseqsets),
    expand('motifs/{reg}/{set}/.done',reg=motseqregions,set=motseqsets),
    expand('gquad_enrich/{reg}/{set}/.done',reg=motseqregions,set=motseqsets),
    # expand('riboWaltz/{sample}/.done',sample=ribosamples),
    # expand('mappability/mappability_{kmer}.bedgraph', kmer = MAPPABILITY_LENGTHS),

MINREADLENGTH=12
MAXREADLENGTH=300
QUALLIM=20
CUTADAPTBIN="~/work/bin/cutadapt"
REMOVE8NBIN="~/work/bin/remove8N_twoarg.pl"
ribowc = '.*(ribo|_Poly|_80S|test|mappability).*'
# for sample in $(find gdrive/RiboSeq_Ribo_Transcriptome/ | tail -n +2 |sed 's/.*\///g' | sed 's/.fastq.gz//g' ); do time=$( echo $sample | cut -d'_' -f 1); rep=$(echo $sample | cut -d'_' -f 2) ; sampname=${time}_ribo_$rep; sampfolder=pipeline/input/$sampname; mkdir -p $sampfolder ; ln -fs  $(readlink -f gdrive/RiboSeq_Ribo_Transcriptome/$sample*) $sampfolder/$sampname.fastq.gz ;done
#for sample in $(find gdrive/RNASeq_Total_Transcriptome/ | tail -n +2 |sed 's/.*\///g' | sed 's/.fastq.gz//g' ); do time=$( echo $sample | cut -d'_' -f 1); rep=$(echo $sample | cut -d'_' -f 2) ; sampname=${time}_total_$rep; sampfolder=pipeline/input/$sampname; mkdir -p $sampfolder ; ln -fs  $(readlink -f gdrive/RNASeq_Total_Transcriptome/$sample*) $sampfolder/$sampname.fastq.gz ;done

#this is a cludge to put in premade files, in a rush
# for sample in $(find ../gdrive/RiboSeq_Ribo_Transcriptome/ | tail -n +2 |sed 's/.*\///g' | sed 's/.fastq.gz//g' ); do time=$( echo $sample | cut -d'_' -f 1); rep=$(echo $sample | cut -d'_' -f 2) ; sampname=${time}_ribo_$rep;pfile="../processed_reads_ribo/${sample}.uniq.qual20-2xNNNN.fastq.gz"; mkdir -p processed_reads/${sampname}; ln -sf  $(readlink -f $pfile) processed_reads/${sampname}/${sampname}.fastq.gz ; touch processed_reads/${sampname}/.done ;done
# for sample in $(find ../gdrive/RNASeq_Total_Transcriptome/ | tail -n +2 |sed 's/.*\///g' | sed 's/.fastq.gz//g' ); do time=$( echo $sample | cut -d'_' -f 1); rep=$(echo $sample | cut -d'_' -f 2) ; sampname=${time}_total_$rep;pfile="../processed_reads_total/${sample}.uniq.qual20.fastq.gz"; mkdir -p processed_reads/${sampname}; ln -sf  $(readlink -f $pfile) processed_reads/${sampname}/${sampname}.fastq.gz ; touch processed_reads/${sampname}/.done ;done
# ll processed_reads/E13_total_1/*
# zhead processed_reads/E13_total_1/*.fastq.gz


# #also make a little test fastq

#things this needs - cutadapt, remove8N.pl, STAR,collapse_reads.pl,seqtk
  
# rule make_test_fastq:
#   input: '../gdrive/RiboSeq_Ribo_Transcriptome','../gdrive/RNASeq_Total_Transcriptome/'
#   output: 'preprocessed_reads/test/test.fastq.gz'
#   shell:r"""
#     set -e
#     mkdir -p preprocessed_reads/test/
#     seqtk sample -s100 ../gdrive/RNASeq_Total_Transcriptome/E13_1.fastq.gz 10000 | gzip > {output}
#     if [ 0 -eq $(gzip -l {output} | awk 'NR==2 {{print $2}}') ]; then rm {output} ; fi

#       """

rule link_in_ref:
  input: REF_orig
  output: REF
  shell:r"""
      ln -fs {REF_orig} {REF}
      """

rule link_in_files:
  input: 'input/{sample}/{fastq}'
  output: 'preprocessed_reads/{sample}/{fastq}'
  run:  
    sample = wildcards['sample']
    fastq = wildcards['fastq']
    shell(r"""
      mkdir -p $(dirname {output})
      ln -sf $(readlink -f input/{sample}/{fastq}) {output}
    """)
    is_over_size(output[0],100)


rule cutadapt_reads:
  input: 'preprocessed_reads/{sample}/{fastq}'
  output: 'cutadapt_reads/{sample}/{fastq}'
  run:
    sampMINREADLENGTH = MINREADLENGTH if not '16bp' in wildcards['sample'] else 0 
    sample = wildcards['sample']

    shell(r"""    #   set -evx
       source activate cutadapt
       mkdir -p cutadapt_reads/{sample}/
       
        zcat {input} \
           | cutadapt \
             -a TGGAATTCTCGGGTGCCAAGG \
            --minimum-length {sampMINREADLENGTH} \
            --maximum-length {MAXREADLENGTH} \
            -q {QUALLIM} - \
        2> cutadapt_reads/{sample}/{wildcards.fastq}.cutadaptstats.txt \
        | gzip  > {output}
    """)
    is_over_size(output[0],100)

rule collapse_reads:
    input: 'cutadapt_reads/{sample}/{fastq}'
    output: 'collapse_reads/{sample}/{fastq}'
    run:
        sample = wildcards['sample']
        colreadstatfile = 'collapse_reads/'+wildcards['sample']+'/'+wildcards['fastq']+'.collreadstats.txt'
        shell(r"""
       set -evx
     
       mkdir -p collapse_reads/{sample}/
     
       zcat {input}  \
         | ~/work/bin/collapse_reads.pl {wildcards.sample} \
         2> {colreadstatfile} \
         | cat > {output}
     """)
        is_over_size(output[0],100)
        assert not "out of mem" in '\n'.join(open(colreadstatfile).readlines())
 
#this finds small filse
 # find collapse_reads/ -name "*.fastq.gz" -size -100M
# find trim_reads/ -name "*.fastq.gz" -size -10M  | xargs ls -latr
# #this finds everything in a certain rule that's less than 10M and then quits
# for i in $(find trim_reads/ -name "*.fastq.gz" -size -10M);do   find . -name $(dirname $i | xargs basename) | grep -v input | grep -v cutadapt; done
rule trim_reads:
    input: 'collapse_reads/{sample}/{fastq}'
    output: 'trim_reads/{sample}/{fastq}'
    wildcard_constraints: sample='.*(ribo|_Poly|_80S).*'
    run:
        sample = wildcards['sample']
        shell(r"""
       set -evx
     
       OUTDIR=$(dirname {output})
       mkdir -p  $OUTDIR
     
       {REMOVE8NBIN} {input} {output}

       gzip -f {output}
       mv {output}.gz {output}

     """)


# rule make_trna_rrna_indices:
#   input: GTF
#   output: touch('filter_reads/tRNA_rRNA_index/tRNA_rRNA_index.done')
#   run:
#     outprefix = output[0].replace('.done','')
#     fafile =outprefix+'.fa'
#     shell(r"""
#       #get rRNAs and tRNAs, rename the tRNAs to exons for GenePRed,
#       #get the gene type out and stick it front of the transcript id
#       #for better names in the fasta
#        cat {GTF} \
#        | grep -Pe 'gene_type\W+rRNA' \
#        | cat - tRNAs.gtf \
#        | sed 's/tRNA/exon/' \
#        | perl -lanpe 's/(transcript_id\W+)(.+gene_type\W+)(\w+)\W/$1$3_$2$3/' \
#        | gtfToGenePred /dev/stdin /dev/stdout \
#        | genePredToBed /dev/stdin /dev/stdout \
#        | bedtools getfasta -s -name -bed - -fi  {REF} \
#        > {fafile}

#        cat  ../ext_data/rRNAseqs/*.fasta >> {fafile} 
#        bowtie2-build {fafile} {outprefix} --threads {threads}
       
#       """)



rule make_trna_rrna_indices:
  input: GTF
  output: touch('filter_reads/tRNA_rRNA_index/tRNA_rRNA_index.done')
  run:
    outprefix = output[0].replace('.done','')
    fafile =outprefix+'.fa'
    shell(r"""
      #get rRNAs and tRNAs, rename the tRNAs to exons for GenePRed,
      #get the gene type out and stick it front of the transcript id
      #for better names in the fasta
      cp ../ext_data/contaminants.fa {fafile}

       cat  ../ext_data/rRNAseqs/*.fasta >> {fafile} 
       bowtie2-build {fafile} {outprefix} -p {threads}
       
      """
)

rule make_bowtie_indices:
  input: REF,RNAFASTA
  output: touch('bowtie_index/.done')
  threads: 8
  run:
    outprefix = output[0].replace('.done','')
    fafile =outprefix+'.fa'
    shell(r"""
      #get rRNAs and tRNAs, rename the tRNAs to exons for GenePRed,
      #get the gene type out and stick it front of the transcript id
      #for better names in the fasta
       bowtie2-build {REF} bowtie_index/ref 
       cp {REF} bowtie_index/ref.fa
       bowtie2-build {RNAFASTA} bowtie_index/transcriptome
       cp {RNAFASTA} bowtie_index/transcriptome.fa
       
      """)


rule testplugin:
      input: 'input/{sample}/{fastq}'
      output: 'trim_reads/{sample,.*test.*}/{fastq}'
      shell:r""" mkdir -p $(dirname {output}) ; ln -sf $(readlink -f {input}) {output}"""

collate_idxscript = "../exploration/collate_idx.R"

rule filter_tRNA_rRNA:
    input: 
      'trim_reads/{sample}/{fastq}',
      'filter_reads/tRNA_rRNA_index/tRNA_rRNA_index.done'  

      # filter_index    
    output: 'filter_reads/{sample}/{fastq,.*fastq.gz}',
    threads: 8
    conda: '../envs/tophat'
    params:
      indexname = lambda wc,input: input[1].replace('.done',''),
      outdir = lambda wc,output: os.path.dirname(output[0])
    shell: r"""
       set -evx

       [ -f {params.outdir} ] && rm -rf {params.outdir}
     
       mkdir -p  {params.outdir}

      bowtie2 \
        -x {params.indexname} \
        -L 20  \
        -p {threads}  \
        -N 0 \
        -U  {input[0]} \
        --un-gz {output[0]} \
        --no-unal \
        2> {output[0]}.alignreport.log > {output[0]}.filtered_reads.sam


        samtools view -bh  {output[0]}.filtered_reads.sam \
        | samtools sort -@ {threads}  > {output[0]}.filtered_reads.bam

      samtools index {output[0]}.filtered_reads.bam
      
      #those which mismatch twice should not be included
      samtools view -hb {params.outdir}/filtered_reads.bam \
      | bamtools filter -tag XM:2-10 -in - -out /dev/stdout \
      | samtools view -H > {output[0]}.mm.sam
      #>> {params.outdir}/unmapped.sam
     
      #group the idx columns stuff is from 
      samtools idxstats {output[0]}.filtered_reads.bam \
      | perl -lanpe 's/^(\S+)_[^_\s]+\t/$1\t/' > {output[0]}.idxtmp

      #Rscript --vanilla {collate_idxscript} {output[0]}.idxtmp {params.indexname}.fa

      samtools stats {output[0]}.filtered_reads.bam > samtools stats {output[0]}.filtered_reads.bam.stats

    """
        # (?# --al-gz $OUTDIR/rRNA_reads  \)
        # --un-gz $OUTDIR/tmp \
 
# print(SAMPLEFASTQDICT)
#this rule is the 'signal spliter where we go from sample to indiv fastqs
rule link_processed_reads:
  input: 
    lambda wc: [fastq.replace('input/','filter_reads/') for fastq in SAMPLEFASTQDICT[wc['sample']]]
  output: touch('processed_reads/{sample,.*(ribo|_Poly|_80S|test|mappability).*}/.done')
  wildcard_constraints: sample=ribowc
  shell:r"""
        mkdir -p processed_reads/{wildcards.sample}
        ln -rifs $(readlink -f {input}) processed_reads/{wildcards.sample}/
    """

rule link_total_fastq:
  input: 'preprocessed_reads/{sample}/{sample}.fastq.gz'
  output: touch('processed_reads/{sample,.*(total|_Input_).*}/.done')
  run:
    sample = wildcards['sample']
    shell(r"""
      set -evx
      mkdir -p processed_reads/{sample}/
      ln -sf $(readlink -f {input}) processed_reads/{sample}/{sample}.fastq.gz
    # ln -rsf $(readlink -f {input}/*) processed_reads/{sample}/{sample}.fastq.gz
    """)

KMERSIZE=6


# rule process_reads_total:
#   input: 'preprocessed_reads/{sample}/'
#   output: touch('processed_reads/{sample,.*total.*}/.done')
#   run:
#     sample = wildcards['sample']
#     shell(r"""
#       set -evx
#         ln -rsf $(readlink -f {input}/*) processed_reads/{sample}/{sample}.fastq.gz
#     """)



      #   samtools view -bh  {output[0]}.filtered_reads.sam \
      #   | samtools sort -@ {threads}  > {output[0]}.filtered_reads.bam

      # samtools index {output[0]}.filtered_reads.bam
      
      # #those which mismatch twice should not be included
      # samtools view -hb {outdir}/filtered_reads.bam \
      # | bamtools filter -tag XM:2-10 -in - -out /dev/stdout \
      # | samtools view -H > {output[0]}.mm.sam
      # #>> {outdir}/unmapped.sam
     
      # #group the idx columns stuff is from 
      # samtools idxstats {output[0]}.filtered_reads.bam \
      # | perl -lanpe 's/^(\S+)_[^_\s]+\t/$1\t/' > {output[0]}.idxtmp

      # Rscript --vanilla {collate_idxscript} {output[0]}.idxtmp {indexname}.fa

      # samtools stats {output[0]}.filtered_reads.bam > samtools stats {output[0]}.filtered_reads.bam.stats

chromsizes = REF.replace('.fa','.chromsizes')
trsizes = GTF.replace('.gtf','.trsizes')

rule gffread:
  input: REF,GTF_orig
  conda: '../envs/gffread'
  output: GTF,CDSGTF,RNAFASTA,CDSFASTA,BED,chromsizes,trsizes
  shell:r""" 
      # set -x
      #with filtering output all sequences
      cat {GTF_orig} \
      | sed -r  's/((transcript_id|gene_id|protein_id|ID=\w+|Parent)\W+\w+)\.[0-9]+/\1/g' \
      > {GTF}

      cat {GFF_orig} \
      | sed -r  's/((transcript_id|gene_id|protein_id|ID=\w+|Parent)\W+\w+)\.[0-9]+/\1/g' \
      > {GFF}
      #needs gff - output exon sequences
      cat {GFF_orig} \
      | sed -r  's/((transcript_id|gene_id|protein_id|ID=\w+|Parent)\W+\w+)\.[0-9]+/\1/g' \
      |  grep -P -e'\texon\t|^\#' | 
      gffread - -F -E -g {REF} -W -w {RNAFASTA} -o /dev/null

      #Note we are now minus the transcript and exon entries for these
      #now make GTF

      #| grep -P -e'\tCDS\t|^\#' 
     #with filtering, output the coding sequences filteirng out the ones that aren't in frame, have a stop codon, are pseudogenes etc.
      
      cat {GFF_orig}  \
        | sed -r  's/((transcript_id|gene_id|protein_id|ID=\w+|Parent)\W+\w+)\.[0-9]+/\1/g' \
        | gffread - -C -V -J --no-pseudo  -F -E -g {REF} \
        -W -w {ANNOBASE}.coding.transcript.fa -x {CDSFASTA} -y {ANNOBASE}.protein.fa -T \
        -o /dev/stdout \
        | awk -v FS="\t" -v OFS="\t" '{{if($3=="CDS"){{$3="exon";print $0}}}}' \
         > {CDSGTF}

      #now make bed
      cat {GTF_orig} | awk '{{print $1,$4,$5,"name",$6,$7}}' > {BED}

      samtools faidx {REF} 
      refname={REF}
      cut -f1,2 {REF}.fai > ${{refname%fa}}chromsizes 

      samtools faidx {RNAFASTA}
      cut -f1,2 {RNAFASTA}.fai  > {trsizes}    
      """



# rule make_seqc_gtf:
#   input: GFF_orig
#   output: SeQC_GTF
#   shell: r"""
#         module load Cufflinks/2.2.1
#       cat {GFF_orig} | \
#       grep -v '#' \
#         | awk '$3 ~ "CDS|exon|start_codon|stop_codon|transcript|UTR"'\
#         | sed -r  's/((transcript_id|gene_id|protein_id|ID=\w+|Parent)\W+\w+)\.[0-9]+/\1/g' \
#         | sed 's/geneID/gene_id/'\
#         | grep -P "transcript_type\W+(protein_coding|rRNA)" \
#         | gffread - -T -o {SeQC_GTF} 
#       """
 
rule make_utrs:
  input: GFF=GFF
  output: fputrs='fputrs.gtf',tputrs='tputrs.gtf'
  # script: 'make_utrfiles.R'
  run:
    shell(r"""
      set -ex
      # module load Cufflinks/2.2.1
      #with filtering output all sequences
      cat {input.GFF}  \
      | awk -v OFS="\t"  '{{if($3=="five_prime_UTR"){{         ;print $0}}}}' \
      | sed -r  's/((transcript_id|gene_id|protein_id|ID=\w+|Parent)\W+\w+)\.[0-9]+/\1/g' \
      | gffread - -T -o {output.fputrs} 

      cat {input.GFF} \
      | awk -v OFS="\t"  '{{if($3=="three_prime_UTR"){{         ;print $0}}}}' \
      | sed -r  's/((transcript_id|gene_id|protein_id|ID=\w+|Parent)\W+\w+)\.[0-9]+/\1/g' \
      | gffread - -T -o {output.tputrs}

     
      """) 

rule fastqc:
     input: 'processed_reads/{sample}/.done'
     output: touch('fastqc/data/{sample}/.done')
     # conda: 'fastqc'
     threads: 4
     log:'fastqc/reports/{sample}/fastqc.log'
     params:
      reads = lambda wc: [fq.replace('input/','processed_reads/') for fq in SAMPLEFASTQDICT[wc['sample']]],
      outdir = lambda wc: 'fastqc/data/'+wc.sample+'/'
     shell: '''
          OUTDIR=$(dirname {output[0]})
          mkdir -p {params.outdir}
          wait $(for i in {params.reads}; do $( fastqc -o {params.outdir} $i ) & done) 
        '''

def get_fastqops(inputdir,read_pattern,lstring='<( zcat ',rstring=')'):
  import glob as glob
  #get our input files, in either paired end or single end form
  assert '-f' in read_pattern
  fastqops = read_pattern.split('-')[1].replace('f ','',1).strip()
  fastqops = glob.glob(inputdir+'/'+fastqops)
  fastqops.sort()
  assert fastqops
  assert all([os.stat(fastq).st_size!=0 for fastq in fastqops])
  fastqops = ' '.join(fastqops)
  
  fastqops = lstring+fastqops+')'

  if '-q' in read_pattern:
    fastqops2 = read_pattern.split('-')[2].replace('q ','',1).strip()
    fastqops2 = glob.glob(inputdir+'/'+fastqops2)
    assert all([os.stat(fastq).st_size!=0 for fastq in fastqops2])
    fastqops2.sort()
    fastqops2 = ' '.join(fastqops2)
    fastqops2 = lstring+fastqops2+')'
    fastqops += ' ' + fastqops2
  return(fastqops)

rule star:
     input:
          fastqs='processed_reads/{sample}/.done',
          STARINDEX=STARINDEX,
          bowtie_index='bowtie_index/.done',
     output:
          done = touch('star/data/{sample,[^/]+}/.done')
     threads: 8
     run:
          input.STARINDEX=input.STARINDEX.replace('.done','')
          markdup = '' if ASSAY_DICT[wildcards['sample']] == 'ribo' else '-m'
          platform = 'NotSpecified'
          inputdir = os.path.dirname(input['fastqs'])
          outputdir = os.path.dirname(output[0])
          read_pattern = READ_PATTERN_DICT[wildcards['sample']]
          fastqops = get_fastqops(inputdir,read_pattern,lstring='<( zcat ',rstring=')')
          repdir = outputdir.replace('data','reports')
          tophatindex =input['bowtie_index'].replace('.done','')
          
          halfthreads = threads/2
          sortmem = str(int(5000/halfthreads))+'M'

          remap = '1' if ASSAY_DICT[wildcards['sample']] == 'ribo' else ''
          remap = ''
          
          sample = wildcards['sample']
          
          outsamrgline =  " \"ID:%s\" \"SM:%s\" \"PL:%s\" " % (sample,sample,platform)
          
          shell(r"""
            set -x
         MY_TMP_DIR=$(mktemp -d)
         #MY_TMP_DIR=/fast/users/dharnet_m/tmp/test
        trap "set -x; rm -rf ${{MY_TMP_DIR}}" EXIT KILL TERM INT HUP

         mkdir -p $MY_TMP_DIR
        mkdir -p $MY_TMP_DIR/star
        mkdir -p $MY_TMP_DIR/tophat2


        
        STAR \
              --genomeDir {input.STARINDEX} \
              --runThreadN {threads} \
              --outSAMunmapped Within \
              --outFilterType BySJout \
              --outMultimapperOrder Random \
              --alignSJoverhangMin 8 \
              --alignSJDBoverhangMin 1 \
              --outFilterMismatchNmax 999 \
              --outFilterMismatchNoverLmax 0.04 \
              --alignIntronMin 20 \
              --alignIntronMax 1000000 \
              --alignMatesGapMax 1000000 \
              --genomeLoad NoSharedMemory \
              --quantMode GeneCounts \
              --outSAMattributes NH HI AS NM MD \
              --outSAMtype BAM  Unsorted\
              --outSAMattrRGline {outsamrgline}\
              --outFileNamePrefix ${{MY_TMP_DIR}}/star/ \
              --outReadsUnmapped Fastx \
              --readFilesIn {fastqops}

          iftophat=
          if [ {remap} && ${{MY_TMP_DIR}}/star/Unmapped.out.mate* ]; then
            iftophat=true
            source activate tophat
            tophat2 \
                -p {threads} \
                -z0 \
                -g 100 \
                --output-dir ${{MY_TMP_DIR}}/tophat2 \
                --library-type fr-unstranded \
                --no-coverage-search \
                --transcriptome-index {tophatindex}/transcriptome \
                {tophatindex}/ref \
                ${{MY_TMP_DIR}}/star/Unmapped.out.mate*

            umapped=${{MY_TMP_DIR}}/tophat2/unmapped.bam
            tmapped=
            
            samtools merge \
              -@ {threads}  -f \
             ${{MY_TMP_DIR}}/all.bam \
             ${{MY_TMP_DIR}}/star/Aligned.out.bam \
             ${{MY_TMP_DIR}}/tophat2/*.bam
          else
            cp ${{MY_TMP_DIR}}/star/Aligned.out.bam ${{MY_TMP_DIR}}/all.bam
          fi
          
         samtools sort \
          -@ {halfthreads}\
          -m {sortmem} \
          -T ${{MY_TMP_DIR}} \
          -o {outputdir}/{sample}.bam \
          ${{MY_TMP_DIR}}/all.bam

        #add the read length as a tag to our bam
        MY_TMP_BAM=$(mktemp)
        samtools view -h {outputdir}/{sample}.bam | awk -vFS='\t' -vOFS='\t' '{{print($0,"Xl:Z:"length($10))}}'  | samtools view -bh > $MY_TMP_BAM
        mv $MY_TMP_BAM {outputdir}/{sample}.bam
      
        samtools index {outputdir}/{sample}.bam

        mkdir -p {repdir}
        samtools stats {outputdir}/{sample}.bam > {repdir}/{sample}.bamstats.txt
        samtools flagstat {outputdir}/{sample}.bam > {repdir}/{sample}.flagstat.log
        samtools idxstats {outputdir}/{sample}.bam > {repdir}/{sample}.idxstats.log
        
        cp  ${{MY_TMP_DIR}}/star/ReadsPerGene.out.tab {outputdir}/ReadsPerGene.out.tab
        cp  ${{MY_TMP_DIR}}/star/{{Log,SJ}}* {repdir}/
        if [ $iftophat ] ;then cp ${{MY_TMP_DIR}}/tophat2/align_summary.txt {repdir} ;fi

          """)
          
rule cutsite_kmers:
  input: 'star/data/{sample}/{sample}.bam',REF
  output: 'cutsequences/{sample}/cutseqs.txt.gz'
  threads: 8
  run:
    chromsizes = 'cutsequences/'+wildcards['sample']+'/chrsizes'
    halfKMERSIZE = KMERSIZE/2
    shell(r"""
        set -evx
        samtools view -H {input} | grep "SQ" | sed -r 's/^@SQ\s+SN://' | sed -r "s/\s+LN:/\t/" > {chromsizes} 
        mkdir -p cutsequences/{wildcards.sample}

        samtools view -bF 0x10 {input[0]} \
          | bedtools bamtobed -i stdin | head -n 100000 \
          | bedtools intersect -a /dev/stdin -b {CDSGTF} -u \
          | awk -v OFS="\t" '{{print $1,$2-{halfKMERSIZE},$2+{halfKMERSIZE}}}' \
          | bedtools getfasta -bed - -fi {REF} -fo /dev/stdout -s \
          | grep -v ">"  | sort | uniq -c  | gzip > {output
       

        samtools view -bF 0x10 {input[0]} \
          | bedtools bamtobed -i stdin | head -n 100000 \
          | bedtools intersect -a /dev/stdin -b {CDSGTF} -u \
          | awk -v OFS="\t" '{{print $1,$3-{halfKMERSIZE},$3+{halfKMERSIZE}}}' \
          |bedtools getfasta -bed - -fi {REF} -fo /dev/stdout -s \
          | grep -v ">"  | sort | uniq -c  | gzip > {output}.right.gz

          # |bedtools slop -i - -b -{halfKMERSIZE} -g {chromsizes}  \
          # |bedtools flank -b {KMERSIZE} -i /dev/stdin -g {chromsizes} \
          # | awk 'NR%2==0' 

        # bedtools bamtobed -i {input[0]} \
        #   |grep '+$' \
        #   |bedtools intersect -a /dev/stdin -b {CDSGTF} -u \
        #   |bedtools slop -i - -b -{halfKMERSIZE} -g {chromsizes}  \
        #   |bedtools flank -b {KMERSIZE} -i /dev/stdin -g {chromsizes} \
        #   |awk 'NR%2==1'  \
        #   |bedtools getfasta -bed /dev/stdin -fi {REF} -fo /dev/stdout -s \
        #   | grep -v ">"  | sort | uniq -c  | gzip > {output}.right.gz
  """)



# TESTSECTION="1:1-1260071"

# rule preprocess_bam:
#     input: get_bam
#     output: "work/{sample}/{sample}.preprocess.bam"
#     shell: r"""
#     samtools view -bh {input} {TESTSECTION} > {output}
#     samtools index {output}
#     """


# rule bwa_mem:
#      input:
#           'processed_reads/{sample}/{sample}.fastq.gz'
#      output:
#           done = touch('star/data/{sample}/.done')
#      threads: 8
#      run:
#           read_pattern = READ_PATTERN_DICT[wildcards['sample']]
#           shell(r"""
#           set -e
#           mkdir -p star/reports/{wildcards.sample}
#           {SCRIPTDIR}/run_my_star.sh -m -@ {threads} -i {input} {read_pattern} -o $(dirname {output}) \
#           -r star/reports/{wildcards.sample} -t {TMPDIR} -l NoSharedMemory -x {STARINDEX} 
#           """)

def get_trackfile(sample,strand,ext):
  return 'bigwigs/'+sample+'/'+strand+'/'+sample+'.'+strand+ext

#TODO strand is a problem here
rule bigwigs:
     input: 
          "star/data/{sample}/.done"
     output:
          done = touch("bigwigs/{sample}/{strand}/{istrans}.done")
     threads: 1
     run:
        bedgraph = get_trackfile(wildcards.sample,wildcards.strand,wildcards.istrans+'.bg')
        bw = get_trackfile(wildcards.sample,wildcards.strand,wildcards.istrans+'.bw')
        bext = BEXTS[wildcards.istrans]
        bam = 'star/data/'+wildcards.sample+'/'+wildcards.sample+bext+'.bam'        
        chromsizes = 'bigwigs/'+wildcards.sample+'/'+wildcards.istrans+'.chrsizes.txt'
        strandsym = STRANDSYMS[wildcards.strand]
        #turns out reversing the y axis breaks bigwig merge....
        # revyaxis = -1 if strandsym is '-' else 1
        revyaxis = 1

        shell(r"""
        set -e
        mkdir -p bigwigs/{wildcards.sample}/{wildcards.strand}
       
        # count="$(cat star/reports/{wildcards.sample}/{wildcards.sample}.bam.bamstats.txt | \
        #  grep -Po "reads mapped:\s+\d+" | \
        #  grep -Po "\d+$" | awk '{{print {revyaxis}*1000000/$0}}' 
        #  )"
        #count=1
        samtools view -H {bam} | grep "SQ" | sed -r 's/^@SQ\s+SN://' | sed -r "s/\s+LN:/\t/" > {chromsizes} 

        bedtools genomecov -ibam {bam} -bg -strand {strandsym} -split -scale $count > {bedgraph}
        bedSort {bedgraph} {bedgraph}
        bedGraphToBigWig {bedgraph} {chromsizes} {bw} 
        s
        """)


def getGroupBigWigs(wildcards):
    strand = wildcards['strand']
    samples = GROUP_samples[wildcards['group']]
    return [get_trackfile(s,strand,wildcards.istrans+'.bw') for s in samples ]


def getGroupBigWigDone(wildcards):
    strand = wildcards['strand']
    samples = GROUP_samples[wildcards['group']]
    return ['bigwigs/'+s+'/'+strand+'/'+wildcards.istrans+'.done' for s in samples ]


rule mergedbigwigs:
     input: getGroupBigWigDone
     output:
          done = touch('mergedbigwigs/{group}/{strand}/{istrans}.done')
     threads: 1
     run:
        chromsizes = 'bigwigs/'+GROUP_samples[wildcards.group][0]+'/'+wildcards.istrans+'.chrsizes.txt' 
        bigwigs = getGroupBigWigs(wildcards)
        libnum = len(bigwigs)
        mergedfilebase="mergedbigwigs/"+wildcards.group+"/"+wildcards.strand+"/"+wildcards.group+"."+wildcards.strand+wildcards.istrans


        shell(r"""
        set -xe
        mkdir -p mergedbigwigs/{wildcards.group}/{wildcards.strand}/
        
        bigWigMerge {bigwigs} /dev/stdout | awk 'BEGIN{{OFS="\t"}}{{$4 = $4 /{libnum} ; print $0 }}' > {mergedfilebase}.bg 
        
        bedSort {mergedfilebase}.bg {mergedfilebase}.bg 

        rm {mergedfilebase}.bg
        """)
# set -e  pipefail; 
#         set -xe
#         module purge
#         module load kenttools/v329-foss-2015a
#         mkdir -p mergedbigwigs/E175_ribo/pos/
        
#         bigWigMerge bigwigs/E175_ribo_1/pos/E175_ribo_1.pos.chr.bw bigwigs/E175_ribo_2/pos/E175_ribo_2.pos.chr.bw /dev/stdout | awk '{$4 = $4 /2  }' > mergedbigwigs/E175_ribo/pos/E175_ribo.pos.chr.bg 
        
#         bedSort mergedbigwigs/E175_ribo/pos/E175_ribo.pos.chr.bg mergedbigwigs/E175_ribo/pos/E175_rib
# o.pos.chr.bg 

#         bedGraphToBigWig \
#           mergedbigwigs/E175_ribo/pos/E175_ribo.pos.chr.bg \
#           bigwigs/E175_ribo_1/.chr.chrsizes.txt \
#           mergedbigwigs/E175_ribo/pos/E175_ribo.pos.chr.bw

#         rm mergedbigwigs/E175_ribo/pos/E175_ribo.pos.chr.bg
rule infer_experiment:
     input:
          'star/data/{sample}/.done'
     output:
          done = touch('infer_experiment/data/{sample}/.done')
     shell:
          r"""
          set -e
          mkdir -p infer_experiment/data/{wildcards.sample}
          
          module purge
          module load RSeQC/2.6.2-foss-2015a-Python-2.7.9
          infer_experiment.py -r {BED} \
          -i star/data/{wildcards.sample}/{wildcards.sample}.bam \
          -q 255 > infer_experiment/data/{wildcards.sample}/{wildcards.sample}.strand_stat.txt
          """


# rule make_salmon_index:
#   threads: 8
#   run:
#     shell(""" salmon index -p 8 --perfectHash -k {KMERSIZE} -t {RNAFASTA} -i salmonindex""")

# rule salmon:
#   input:
#     'star/data/{sample}/.done'
#      output:
#       done = touch('salmon/data/{sample}/.done')
#      threads: 8
#      GTF = GTF_DICT[wildcards['sample']]
#           protocol = PROTOCOL_DICT[wildcards['sample']]
#           if (protocol == 'no'):
#                protocol = 0
#           elif (protocol == 'yes'):
#                protocol = 1
#           elif (protocol == 'reverse'):
#                protocol = 2
#           else:
#                sys.exit('Protocol not known!')

#            library = LIBRARY_DICT[wildcards['sample']]

#           if (library == 'PAIRED'):
#             if protocol == ''
#                FORMATSTRING = 'I'
#           else:
#                library = ''

#           TARGETFASTA

#           FORMATSTRING = format1+format2+format3

#           shell(r"""
#           set -ex
#           mkdir -p salmon/reports/{wildcards.sample}
#           mkdir -p salmon/data/{wildcards.sample}
#       salmon quant \
#       -l {FORMATSTRING} \
#       --seqBias --gcBias \
#       -g transcript_gene_map.tsv \
#       -t {TARGETFASTA} \
#       --fldMean 433 --fldSD 186 -a star/data/E13_total_1/E13_total_1.star_transcript.bam --output salmon/data/E13_total_1/E13_total_1

#           {SCRIPTDIRsalmon -m -@ {threads} -i {input} {read_pattern} -o $(dirname {output}) \
#           -r star/reports/{wildcards.sample} -t {TMPDIR} -l NoSharedMemory -x {STARINDEX} 
#           """)


     
transcript_gene_map = 'transcript_gene_map.tsv'
gene_transcript_map = 'gene_transcript_map.tsv'

rule make_gene_transcript_map:
  input: GTF
  output: gene_transcript_map,transcript_gene_map
  run:
    shell(r"""
    cat {input} \
      | grep -Pe'\ttranscript\t'  \
      | perl -lane '/transcript_id\W+([\w\.]+)/;$t=$1; $g=/gene_id\W+([\w\.]+)/;$g=$1;print($g,"\t",$t)' \
      | sort | uniq \
      > {gene_transcript_map}

    cat {gene_transcript_map} \
      | awk '{{print $2,$1}}' > {transcript_gene_map}
    """)
    is_nonempty(gene_transcript_map)

rule rsemref:
  input:
    GTF=GTF,
    REF=REF,
    gene_transcript_map=gene_transcript_map
  threads: 10
  output:
    touch(RSEMINDEXFOLD+'/.done')
  run:
    shell(r"""

      set -ex      

      mkdir -p {RSEMINDEXFOLD}      

      rsem-prepare-reference -p {threads} --gtf {input.GTF} \
                              --transcript-to-gene-map {input.gene_transcript_map} \
                              --star  \
                              {input.REF} \
                              {RSEMINDEXFOLD}/{ANNOBASE} 
      """) 


rule rsem:
     input:
          'star/data/{sample}/.done',
          RSEMINDEXFOLD+'/.done'
     output:
          done = touch('rsem/data/{sample}/.done')
     threads: 8
     run:
        frag_length_mean = FRAG_LENGTH_MEAN_DICT[wildcards['sample']]
        indexname = RSEMINDEXFOLD+'/'+(os.path.basename(GTF).replace('.gtf',''))

        frag_length_sd = FRAG_LENGTH_SD_DICT[wildcards['sample']]

        shell(r"""
        set -e

        rm -rf $(dirname {output})
        mkdir -p $(dirname {output})
        rm -rf rsem/reports/{wildcards.sample}
        mkdir -p rsem/reports/{wildcards.sample}


        {SCRIPTDIR}/run_my_rsem.sh -@ {threads} \
        -b star/data/{wildcards.sample}/{wildcards.sample}.star_transcript.bam \
        -o $(dirname {output}) -r rsem/reports/{wildcards.sample} -t {TMPDIR} \
        -x $(echo {RSEMINDEXFOLD}/*grp | sed s/.grp//) \
        -f {frag_length_mean} -d {frag_length_sd} \
        # &> rsem/reports/{wildcards.sample}/{wildcards.sample}.rsem.log
        """)


rrna_intervals = 'qc/picard_rrna_intervals.txt'
refflat = 'qc/'+ANNOBASE+'.refflat'
testsample = 'test_16bpRNA'#we need a bamheader for testing

rule make_picard_files:
  input: GTF,'star/data/'+testsample+'/.done'
  output: intervals=rrna_intervals,refflat=refflat
  shell:r"""
         source activate picard
         samtools view -H star/data/{testsample}/{testsample}.bam > {output.intervals}

         grep -Pe 'gene_type..rRNA.' {input[0]} \
         | awk '$3 =="transcript"' \
         | cut -f 1,4,5,7,9 \
         | perl -lane ' /transcript_id "([^"]+)"/ or die "notranscript_id on $."; print join "\t", (@F[0,1,2,3], $1) ' \
         | sort -k1V -k2n -k3n  - >> {output.intervals}
        
        gtfToGenePred -geneNameAsName2 {GTF} {GTF}.genepred
        cat {GTF}.genepred | awk -vOFS="\t" '{{print $1,$0}}' > {output.refflat}

  """

rule qc:
     input:
          fastqc='fastqc/data/{sample}/.done',
          star='star/data/{sample}/.done',
          refflat = refflat,
          rrna_intervals = rrna_intervals,
     output:
          done=touch('qc/data/{sample}/.done'),
     run:
        singleendflag = '' if LIBRARY_DICT[wildcards['sample']] == 'PAIRED' else ' -singleEnd' 
        bamfile = 'star/data/'+wildcards['sample']+'/'+wildcards['sample']+'.bam' 
        shell(
          r"""
          set -e
          set -xv
        source activate picard
          
        OUTDIR=$(dirname {output.done})
        mkdir -p qc/reports/{wildcards.sample}/

        {SCRIPTDIR}/read_statistic_report.sh \
         -l star/reports/{wildcards.sample}/Log.final.out  \
         -g $(dirname {input.fastqc}) \
         -o ${{OUTDIR}}/read_alignment_report.tsv \
         &> qc/reports/{wildcards.sample}/{wildcards.sample}_qc.log 

         picard CollectRnaSeqMetrics \
          I={bamfile} \
          O=${{OUTDIR}}/{wildcards.sample}_picard_qc.txt \
          REF_FLAT={refflat} \
          STRAND=FIRST_READ_TRANSCRIPTION_STRAND \
          RIBOSOMAL_INTERVALS={rrna_intervals}
        
        #picard CollectAlignmentSummaryMetrics \
        #  INPUT={bamfile} \
        #  OUTPUT=${{OUTDIR}}/{wildcards.sample}.picard.alignmentmetrics.txt \
        #  R={REF}

      {SCRIPTDIR}/read_duplication.sh \
        -i {bamfile} \
        -o ${{OUTDIR}}/duplication/ \
        &> qc/reports/{wildcards.sample}/{wildcards.sample}_qc.log 


          """)

multiqcscript = '~/work/Applications/MultiQC/scripts/multiqc'


rule multiqc:
  input:
      expand("fastqc/data/{sample}/.done", sample = samples),
      expand("star/data/{sample}/.done", sample = samples),
      expand("qc/data/{sample}/.done", sample = samples),
      # expand("tophat2/data/{sample}/.done", sample = samples),
      [f.replace('input','filter_reads') for f in  ribofastqs],
      'sample_file.txt'
 
  output:
    'multiqc/multiqc_report.html'
  run:
    reportsdirs = list(input)
    reportsdirs=[s.replace('star/data','star/reports') for s in reportsdirs]
    reportsdirs=[s.replace('tophat2/data','tophat2/reports') for s in reportsdirs]
    reportsdirs=[os.path.dirname(s) for s in list(reportsdirs)]
    shell(r"""
      cat sample_file.txt | sed 's/.fastq.gz//g' | sed 's/\t.*\//\t/g' \
      | awk -vOFS='\t' 'BEGIN{{print "fastqname","samplename"}}{{sumsamp[$1] = sumsamp[$1]+1;print $2,$1"_fq"sumsamp[$1]}}' \
      > multiqc/samplenames.txt

      {multiqcscript} {reportsdirs} -fo $(dirname {output[0]}) -c multiqc_config.yaml --sample-names multiqc/samplenames.txt
      """)

    
     
rule dupradar:
    input:
        'star/data/{sample}/.done'
    output:
        touch('dupradar/data/{sample}/.done')
    run:
        library = LIBRARY_DICT[wildcards['sample']]
        paired = 'TRUE' if library == 'PAIRED' else 'FALSE'

        protocol = PROTOCOL_DICT[wildcards['sample']]
        map_protocol = { 'no': 0, 'yes': 1, 'reverse': 2 }
        stranded = map_protocol[protocol]

        shell(r"""

        outdir=$(dirname {output})
        id={wildcards.sample}
        bam=$(dirname {input})/${{id}}.bam
        threads=8

        mkdir -p $outdir
        
        Rscript --vanilla {SCRIPTDIR}/dupradar.R \
            $bam \
            $id \
            {GTF} \
            {stranded} \
            {paired} \
            $threads \
            $outdir
        """)


rule make_kallisto_index:
  input: 
    RNAFASTA,
    CDSFASTA
  output: KINDEX,KINDEXCDS,'sizes.genome'
  shell:
    r"""
    set -ex
    export LD_LIBRARY_PATH=/fast/users/harnettd_c/miniconda3/lib/:$LD_LIBRARY_PATH 
    mkdir -p kallistoindex

    samtools faidx {REF}
    cut -f1,2 {REF}.fai > sizes.genome

    {kallistobin} index -i {KINDEXCDS} -k {KMERSIZE} {RNAFASTA}
    {kallistobin} index -i {KINDEX}  -k {KMERSIZE} {CDSFASTA}
    """

    
    
rule kallisto:
  input:
    'processed_reads/{sample}/.done',
    KINDEX,
    KINDEXCDS,
    'sizes.genome'
  output:
    done = touch('kallisto/data/{sample}/.done')
  threads: 8
  run:
    frag_length_mean = FRAG_LENGTH_MEAN_DICT[wildcards['sample']]
    frag_length_sd = FRAG_LENGTH_SD_DICT[wildcards['sample']]
    read_pattern = READ_PATTERN_DICT[wildcards['sample']]
    kallistoindex = KINDEXDICT[wildcards['sample']]
    protocol = PROTOCOL_DICT[wildcards['sample']]


    if (protocol == 'no'): protocol = ''
    elif (protocol == 'yes'):
      protocol = '--fr-stranded'
    elif (protocol == 'reverse'):
      protocol = '--rf-stranded'
    else:
      sys.exit('Protocol not known!')
    fastqfold = input[0].replace('.done','')

    shell(r"""
    set -ex
    rm -rf kallisto/*/{wildcards.sample}/
    mkdir -p kallisto/reports/{wildcards.sample}

    export LD_LIBRARY_PATH=$HOME/miniconda3/lib/:$LD_LIBRARY_PATH 

    {SCRIPTDIR}/run_my_kallisto.sh  -x {kallistoindex} -c \
    -@ {threads} \
    -s {protocol} \
    -b 1 \
    -t {TMPDIR} -i {fastqfold} {read_pattern} \
    -l {frag_length_mean} -d {frag_length_sd} \
     -o $(dirname {output}) 
    """)
    # -p  \
    # -m sizes.genome \
    # -a \
    # -g {GTF}\


RIBOTAPERDIR="$HOME/Applications/ribotaper-1.3.1a/scripts/"
RIBOTAPERANNOTDIR="Ribotaper_annot"

rule make_ribotaper_annot:
  input: GTF_orig = '/fast/projects/cubit/0.12.0/static_data/annotation/GENCODE/M12/GRCm38/gencode.vM12.annotation.gtf'
  output: touch(RIBOTAPERANNOTDIR+"/.done")
  run:
    shell(r"""
      set -ex

      mkdir -p {RIBOTAPERANNOTDIR}
      {RIBOTAPERDIR}/create_annotations_files.bash $(readlink -f {input.GTF_orig}) $(readlink -f {REF}) 'false' 'false' {RIBOTAPERANNOTDIR}
      """)

rule make_ribotaper_metaplots:
  input:
    annodir = RIBOTAPERANNOTDIR+"/.done",
    ribobam = "star/data/{sample}/{sample}.bam"
  output:
    touch('ribotapermetaplots/{sample}/.done')
  run:
    startstopbed=RIBOTAPERANNOTDIR+'/start_stops_FAR.bed'
    shell(r"""
      mkdir -p ribotapermetaplots/{wildcards.sample}
      bed=$(readlink -f {startstopbed})
      bam=$(readlink -f {input.ribobam})
      cd ribotapermetaplots/{wildcards.sample}
      rm -rf metaplots
      {RIBOTAPERDIR}/create_metaplots.bash $bam $bed {wildcards.sample}
      """)


skribo_rnafold = 'skribo_rnafold'
rnafoldbin = '/fast/users/harnettd_c/Applications/scikit-ribo/scikit_ribo/'
skindexfold = 'skribo_index'
rule scikit_rnafold:
  input: RNAFASTA,
  output: skribo_rnafold
  run:
    shell(r"""
      set -xv
    python {rnafoldbin}//call_rnafold.py\
    -f shortname_my_gencode.vM12.annotation.transcript.fa \
    -r {rnafoldbin} \
    -p testskribo \
    -o {skribo_rnafold} 
    """)

    # gtf=<gtf-file-location>
    # fasta=<ref-genome-fasta-location>
    # bam=<bam-file-location> # from STAR
    # rnafold=<path to the Rnafold file> # the file generated by call_rnafold.py
    # prefix=<prefix-to-use> # defined by user
    # index_folder=<index-output-folder>
    # RNA=<RNAseq-TPM-file-location> # gene level TPM generated with salmon or kallisto
    # output=<output-folder>
    # unmap=<unmap-regions>
#-g {GTF_orig} \

rule scikit_ribo_build:
  input: skribo_rnafold
  output: touch('skindexfold/{sample}/.done')
  run:
    shell(r"""

    mkdir -p {skindexfold}/{wildcards.sample}
    scikit-ribo-build.py \
    -g test.gtf \
    -f {REF} \
    -p testskribo \
    -r {skribo_rnafold} \
    -t kallisto/data/{wildcards.sample}/out.d/abundance.tsv\
    -o {skindexfold}/{wildcards.sample}
""")


rule scikit_ribo:
  input: 'skindexfold/{sample}/.done'
  output: touch('scikitribo/{sample}/.done')
  run:
    shell(r"""

    scikit-ribo-run.py \
    -i star/data/{wildcards.sample}/{wildcards.sample}.bam \
    -f {skindexfold} \
    -p testskribo \
    -o scikitribo/{wildcards.sample}/ """)


#this is going to count reads in each library over 5'UTRS, CDS, and 3' UTRs
rule readlenfilt:
  input: ALIGNER_TO_USE+'/data/{sample}/{sample}.bam'
  output:  'readlenfilt/data/{sample}/{readrange}/readlenfilt.bam'
  threads:4
  run:
    minreadlen,maxreadlen = wildcards['readrange'].split('_')
    readrangebam = output[0]
    shell(r"""
          set -ex
          samtools view -h $(dirname {input})/{wildcards.sample}.bam \
          | awk '((length($10) >= {minreadlen})&&(length($10) <= {maxreadlen})) || $1 ~ /^@/' \
          | samtools view -F 4 -S -b - > {readrangebam}.tmp
          #

         MY_TMP_DIR=$(mktemp -d)

         samtools sort \
          -@ {threads}\
          -m 2G \
          -T ${{MY_TMP_DIR}} \
          -o {readrangebam} \
          {readrangebam}.tmp

          samtools index {readrangebam}
      """)
       

rule feature_counts_readrange:
     input:
          'tputrs.gtf','fputrs.gtf',bam = 'readlenfilt/data/{sample}/{readrange}/readlenfilt.bam',GTF=GTF
     output:
          'feature_counts_readrange/data/{sample,[^/]+}/{gcol_generegion}/{readrange}/feature_counts'
     threads: 2
     log: r"""feature_counts_readrange/reports/{sample}/{gcol_generegion}/{readrange}/feature_counts.log"""
     run:
          groupcol,generegions = wildcards['gcol_generegion'].split('__')
          minreadlen,maxreadlen = wildcards['readrange'].split('_')
          if (generegions in ['CDS']): GTF,featuretype = input.GTF,'CDS'            
          elif (generegions in ['tputrs']): GTF,featuretype = tputrs,'exon'
          elif (generegions in ['fputrs']): GTF,featuretype = fputrs,'exon'
          else: GTF = GTF

          
          protocol = PROTOCOL_DICT[wildcards['sample']]
          if (protocol == 'no'):
               protocol = 0
          elif (protocol == 'yes'):
               protocol = 1
          elif (protocol == 'reverse'):
               protocol = 2
          else:
               sys.exit('Protocol not known!')

          library = LIBRARY_DICT[wildcards['sample']]

          if (library == 'PAIRED'):
               library = '-p'
          else:
               library = ''

          countmultimappers = ' ' 
          
          if (generegions=='tRNAs'):
            featuretype = 'tRNA'
            countmultimappers = '-M --fraction'
          

          sample = wildcards['sample']
          
          shell(r"""
          set -ex
          mkdir -p feature_counts_readrange/data/{sample}/{generegions}/{wildcards.readrange}/
          mkdir -p feature_counts/reports/{wildcards.sample}/
          
          featureCounts \
            -O \
            -T {threads} \
            -t {featuretype} -g {groupcol} \
            -a {GTF} \
            -s {protocol} {library} {countmultimappers} \
            -o {output} \
            {input.bam}             

            # &> feature_counts/reports/{wildcards.sample}/{wildcards.sample}.feature_counts.log

          """)

def get_readrange(wc):
    if wc['sample'] in ribosamples:
          selregion='gene_id__CDS'
          selreadrange='25_31'
    else:
          selregion='gene_id__CDS'
          selreadrange='20_1000'
    fcountfile = 'feature_counts_readrange/data/'+wc['sample']+'/'+selregion+'/'+selreadrange+'/feature_counts'
    return   fcountfile

rule feature_counts:
     input:
          get_readrange,
          ALIGNER_TO_USE+'/data/{sample}/{sample}.bam',GTF,'tputrs.gtf','fputrs.gtf',
     output:
          'feature_counts/data/{sample,[^/]+}/feature_counts'
     threads: 2
     run:       
        bamfile = ALIGNER_TO_USE+'/data/'+wildcards['sample']+'/'+wildcards['sample']+'.bam' 

        shell(r"""

          mkdir -p feature_counts/reports/{wildcards.sample}
          mkdir -p feature_counts/data/{wildcards.sample}

          head -n2 {input[0]} \
          | tail -n1 \
          | awk -v FS="\t" -v OFS="\t"  '{{$7= "{bamfile}";print $0}}' \
          >    feature_counts/data/{wildcards.sample}/feature_counts

          tail -n+3 {input[0]} \
          >>   feature_counts/data/{wildcards.sample}/feature_counts

          cp {input[0]}.summary {output[0]}.summary
          """)

# #this is going to count reads in each library over 5'UTRS, CDS, and 3' UTRs
rule aggregate_feature_counts:
  input : expand("feature_counts/data/{sample}/feature_counts", sample = samples),
  output: 'feature_counts/all_feature_counts'
  run:
    fcountfiles = list(input)
    shell(r""" 

       #( (sed '2q;d' {fcountfiles[0]} | cut -f1 && tail -n+3 {fcountfiles[0]}| sort -k1 | cut -f1) > {output})
       tail -n+3 {fcountfiles[0]}| sort -k1 | cut -f1 > {output}
       
       #now for eahc fcount table, join it to the ids
       for fcountfile in $(echo {fcountfiles}); do

          tail -n+3 $fcountfile| sort -k1 | cut -f1,7 | join {output} - | sort -k1 > {output}tmp
          mv {output}tmp {output}
       
       done

      echo "feature_id {samples}" | cat - {output} > {output}tmp
      mv {output}tmp {output}
    
      """)



riboqcanno='riboqc/'+Path(GTF).name.replace('.gtf','.matchchrs.gtf')+'_Rannot'

# rule make_riboqc_anno:
#    input : GTF,REF,REF+'.fai'
#    output: touch('riboqc/annot.done'),riboqcanno
#    params:
#     speciesname = lambda wc,input: Path(input[1]).name.replace('.fa','') + '.foo',
#     annobase = lambda wc,input: input[0].replace('.gtf',''),
#     outdir = lambda wc,output: output[0].replace('annot.done',''),
#     twobitfile = lambda wc,input: input[1].replace('.fa','.twobit'),
#     gtfmatchchrs = lambda wc,input: input[0].replace('.gtf','.matchchrs.gtf')
#    shell:r"""
#          set -x`
#      #faToTwoBit {REF} {params.twobitfile}

#      samtools faidx {REF}

#     awk -vOFS="\t" '{{print $1,0,$2}}'  {REF}.fai | bedtools intersect -b -  -a {GTF} > {params.gtfmatchchrs}

#      mkdir -p riboqc
#      R -e ' library(RiboseQC); prepare_annotation_files("{params.outdir}","{params.twobitfile}","{params.gtfmatchchrs}","{params.speciesname}","{params.annobase}",forge_BS=FALSE,genome_seq=FaFile("{REF}")) '

#  """
RIBOSEQCPACKAGE = '~/work/Applications/RiboseQC'
# RIBOSEQCPACKAGE = config['RIBOSEQCPACKAGE']

rule make_riboseqc_anno:
  input : 
    GTF,
    REF,str(REF)+'.fai'
  output: Path(GTF).with_suffix('.matchchrs.gtf_Rannot')
  params:
    annobase = Path(GTF).name.replace('.gtf','').replace('.gz',''),
    gtfmatchchrs = lambda wc,input: input[0].replace('.gtf','.matchchrs.gtf')
  shell:r"""
    set -x
    awk -vOFS="\t" '{{print $1,0,$2}}' {REF}.fai | bedtools intersect -b - -a {GTF} > {params.gtfmatchchrs}
    mkdir -p $(dirname {output[0]})
    R -e 'devtools::load_all("{RIBOSEQCPACKAGE}");args(prepare_annotation_files) ;prepare_annotation_files(annotation_directory=".",gtf_file="{params.gtfmatchchrs}",annotation_name="{params.annobase}",forge_BS=FALSE, genome_seq=FaFile("{REF}"))'
 """



rule make_chrnamefile:
 input: GTF
 output: 'chrnames.txt'
 shell:r""" cut -d$'\t' -f1 {input} | uniq | sort | uniq | grep -v '#' > {input}"""


# rule run_riboqc:
#    input: 'riboqc/annot.done','star/data/{sample}/.done',REF,GTF,riboqcanno,'riboWaltz/data/{sample}/offsets.tsv'
#    output: touch('riboqc/data/{sample}/.done'),'riboqc/data/{sample}/_for_SaTAnn','riboqc/reports/{sample}/riboqcreport.html'
#    # conda: '../envs/riboseqc'
#    params:    
#      annofile = lambda wc,input: input[0].replace('annot.done',Path(GTF).name.replace('.gtf','.matchchrs.gtf_Rannot')),
#      bamfile = lambda wc: 'star/data/'+wc['sample']+'/'+wc['sample']+'.bam',
#      outname = lambda wc,output: output[0].replace('.done',''),
#      report_file = lambda wc: 'riboqc/reports/'+wc['sample']+'/'+'riboqcreport.html',

#    shell:r"""
#          set -x
#          mkdir -p {params.outname}
#          mkdir -p riboqc/reports/{wildcards.sample}
#          R -e 'library(RiboseQC);RiboseQC::RiboseQC_analysis("{params.annofile}", bam="{params.bamfile}",offsets_df=readr::read_tsv("{params.ribowaltztsv}"),dest_names="{params.outname}", genome_seq = "{REF}", report_file="{params.report_file}")'
     
#      """


rule run_riboseqc:
   input: Path(GTF).with_suffix('.matchchrs.gtf_Rannot'),bam=ALIGNER_TO_USE+'/data/{sample}/{sample}.bam'
   output: touch('riboseqc/data/{sample}/.done'),'riboseqc/data/{sample}/_for_SaTAnn','riboseqc/reports/{sample}/riboseqcreport.html'
   threads: 4
   params:
     annofile = lambda wc,input: input[0].replace('annot.done',Path(GTF_orig).name.replace('.gtf','.matchchrs.gtf_Rannot')),
     outname = lambda wc,output: output[0].replace('.done',''),
     report_file = lambda wc: 'riboseqc/reports/'+wc['sample']+'/'+'riboseqcreport.html',
   shell:r"""
         set -x
         mkdir -p {params.outname}
         mkdir -p riboseqc/reports/{wildcards.sample}
         R -e 'devtools::load_all("{RIBOSEQCPACKAGE}");RiboseQC::RiboseQC_analysis("{params.annofile}", bam="{input.bam}",rescue_all_rls=TRUE,dest_names="{params.outname}", genome_seq = "{REF}", report_file="{params.report_file}")'
     """

rule get_offset_model:
  input:
    bam = 'star/data/{sample}/{sample}.bam',
    gtf = GTF,
    REF = REF,
  output: 'seqshift_reads/data/{sample}/seqshiftmodel.rds'
  shell:r"""Rscript ../src/R/Psite_offsets/offsets3.R {input} $(dirname {output})"""

rule quant_riboseq:
  input:
    bam = 'star/data/{sample}/{sample}.bam',
    gtf = GTF,
    REF = REF,
    shiftmodel = 'seqshift_reads/data/{sample}/seqshiftmodel.rds',
  threads:10
  output: 'riboseq_quant/data/{sample}/segment_counts_df.tsv'
  shell:r"""Rscript ../src/R/Psite_offsets/segment_counts.R {input} $(dirname {output})"""


rule run_satann:
  input : 'riboseqc/data/{sample}/.done',Path(GTF).with_suffix('.matchchrs.gtf_Rannot')
  output: touch('SaTAnn/{sample}/.done')
  params: 
    for_satannfile = lambda wc,input: 'c("'+('","'.join(['riboseqc/data/'+s+'/''_for_SaTAnn' for s in [wc['sample']] ]))+'")',
    annofile = lambda wc,input: 'riboseqc/'+Path(GTF).name.replace('.gtf','.matchchrs.gtf_Rannot'),
    outputdir = lambda wc,output: output[0].replace('.done','')
  threads: 10
  shell:r"""
    set -ex
      mkdir -p {params.outputdir}

      R -e 'devtools::load_all("/fast/AG_Ohler/Eelco/RP/Senescence_ribo/applications/SaTAnn");run_SaTAnn(for_SaTAnn_file = {params.for_satannfile},annotation_file = "{params.annofile}",genome_seq = "{REF}", n_cores = {threads},prefix="{params.outputdir}") '
        
      """


ms_total_file='/fast/groups/cubi/projects/2017-10-10_cortexomics/gdrive/cortexomics_ms_total/325_new_2_all_log2_LFQ_n7464.txt'
ms_spec_file='/fast/groups/cubi/projects/2017-10-10_cortexomics/gdrive/cortexomics_ms_cyt+80+Poly/proteinGroups.txt'




rule make_id_table:
  input: GTF
  output: 'ids.txt'
  shell: r"""R -e 'library(tidyverse,quiet=T); library(rtracklayer,quiet=T);import("{input}") %>%mcols%>%as.data.frame%>% select(gene_id,gene_name)%>%distinct%>%write.table("{output}", col.names=TRUE, row.names=FALSE)'"""

rule create_ms_exprtables:
  input: ms_total_file,ms_spec_file
  output: touch('ms_tables/.done')
  shell: r"""Rscript ../src/load_data/create_ms_exprtables.R {input[0]} {input[1]} $(dirname {output})"""

DISPDIFF = 0

rule run_ribodiff:
  input: 'feature_counts/all_feature_counts'
  # conda: '../envs/ribodiff_filt'
  output: touch('ribodiff/.done')
  shell: r"""source activate ribodiff ; Rscript ../src/R/TE_change/run_ribodiff.R {input} {DISPDIFF} $(dirname {output})"""

DISPDIFF = 0

rule run_xtail:
  input: 'feature_counts/all_feature_counts','SaTAnn/uORFs.feature_counts'
  output: touch('xtail/.done')
  shell: r"""Rscript ../src/R/TE_change/run_xtail.R {input} $(dirname {output})"""

MS_MEASURE_TO_USE = 'LFQ'
mstbl = 'ms_tables/ms_'+MS_MEASURE_TO_USE+'_total_ms_tall.tsv'



#needs to be integrate exprdata2 nwo
rule integrate_exprdata:
  input: 'CDS_rna_counts',mstbl,expand('riboseq_quant/data/{sample}/segment_counts_df.tsv',sample=ribosamples)
  output: 'exprdata/transformed_data.txt','exprdata/cent_scaled_exprdata.txt','exprdata/designmatrix.txt','exprdata/allcounts_snorm.tsv'
  shell: r"""mkdir -p exprdata;
  Rscript ../src/R/Load_data/integrate_exprdata.R {input[0]} {input[1]} {output}"""

rule get_limma_fold_changes:
  input: 'exprdata/transformed_data.txt','exprdata/designmatrix.txt'
  output: 'exprdata/limma_fold_changes.txt','exprdata/limma_fold_changes.txtfull.txt'
  shell: r"""Rscript ../src/Modeling/get_limma_fold_changes.R {input[0]} {input[1]} {output[0]}"""

rule perform_clustering:
  input: 'exprdata/{clusterdata}.txt',
  output: touch('clusters/{clusterdata}/{clustermethod}/.done')
  shell: r"""Rscript ../src/Modeling/cluster_{wildcards.clustermethod}.R {input} $(dirname {output}) 10"""

rule comp_ribodiff_limma:
  input: 'ribodiff/.done','exprdata/limma_fold_changes.txtfull.txt'
  output: 'ribodiff_limma_comp/.done'
  shell: r""" ../src/Modeling/comp_ribodiff_limma.R $(dirname {input[0]}) $(dirname {input[1]}) $(dirname {output}) """

rule ribodiff_filt:
  input: 'ribodiff/.done','ids.txt','exprdata/cent_scaled_exprdata.txt','exprdata/limma_fold_changes.txt',
  output: 'exprdata/{clusterdata,.+_ribodfilt.txt}'
  shell: r"""Rscript ../src/R/TE_change/ribodiff_filter.R {input}"""

rule assess_clustering:
  input: 'clusters/{clusterdata}/{clustermethod}/.done'
  output: 'cluster_assessment/{clusterdata}/{clustermethod}/.done'
  shell: r"""Rscript ../src/Modeling/assessclusters.R $(dirname {output}) $(dirname {output})"""

rule reg_seqs:
  input: genome=REF,tputrs='tputrs.gtf',seqs='ribodiff/riboseqres_P0.txt'
  output: 'regulated_sequences.fa'
  shell: r"""
    R -e   'library(magrittr);library(GenomicRanges);library(data.table);myfasta <- Rsamtools::FaFile("{input.genome}") ; myutrs <- rtracklayer::import("{input.tputrs}") ; diffIDs <-  "{input.seqs}"%>% fread %>% dplyr::filter(padj<0.05) %>% .$geneID; myutrs %>% subset(gene_id %in% diffIDs) %>%split(.,.$gene_id) %>% reduce %>% {{GenomicFeatures::extractTranscriptSeqs(x=myfasta,.)}} %>% {{Biostrings::writeXStringSet(., "{output[0]}")}}'
  """

#


#Things we want to search for motifs - anything that's translationally reuglated
#later, clusters

rule motseqs:
  input:'ribodiff/.done','exprdata/allcounts_snorm.tsv',GFF,REF,  
  output: 
    touch('motseqs/.done'),
    expand('motseqs/{reg}/{set}/{set}.fa',reg=motseqregions,set=motseqsets)
  run:
    input[0]=input[0].replace('.done','')
    shell(r"""
      Rscript ../exploration/createseqmot_background.R {input} $(dirname {output})
    """)

rule motifs:
  input: 'motseqs/{reg}/{set}/{set}.fa'
  output: touch('motifs/{reg}/{set}/.done')
  run: 
    outputdir = os.path.dirname(output[0])
    background = input[0].replace('.fa','_background.fa')
    nontransbackground = os.path.dirname(os.path.dirname(input[0]))+'/nontransregged.fa'
    shell(r"""set -x
      source activate meme
      mkdir -p {outputdir}_dreme {outputdir}_dreme_rback {outputdir}_dreme_ntback

      dreme-py3 -e 0.05 -mink 4 -maxk 12 -p {input} -norc  -oc {outputdir}_dreme_ntback  -n {nontransbackground} 2>&1 > {outputdir}_dreme_rback/enlist.txt

      dreme-py3 -e 0.05 -mink 4 -maxk 12 -p {input} -norc  -oc {outputdir}_dreme -n {background} 2>&1 > {outputdir}_dreme/enlist.txt

      # dreme-py3 -e 0.05 -mink 4 -maxk 12 -p {input} -norc  -oc {outputdir}_dreme_rback  2>&1 > {outputdir}_dreme_rback/enlist.txt
    """)
    # shell(r"""findMotifs.pl -p {threads} {input} fasta {outputdir} -fastaBg {background} """)
    # shell(r"""findMotifs.pl -p {threads} {input} fasta {outputdir}_randbackground  """)

# shell(r"""findMotifs.pl {input} fasta {outputdir} -fasta {background} –rna -minw5 -mod zoops -maxsize  20000000 -nmotifs 5 -evt 0.01""")

#for i in  ../ext_data/mus_musculus_rnamotifs/pwms_all_motifs/M*.txt ; do cat $i | tail -n+2 | cut -d$'\t' -f2,3,4,5 | matrix2meme -n 100 -pseudo 1 > $i.meme ; done
# fimo ../ext_data/mus_musculus_rnamotifs/pwms_all_motifs/M100_0.6.txt.meme  motseqs/CDS/translregged/translregged.fa
# fread('ext_data/tops_yamashita_etal_2008/supp_data_table_2.csv')%>%filter(`known TOP`=='known')%>%.$seq%>%str_split(',')%>%unlist%>%keep(~ .!='')%>%DNAStringSet%>%consensusMatrix%>%.[1:4,]%>%apply(2,function(x)x/sum(x))%>%t%>%as.data.frame%>%mutate(Pos=seq_len(n()))%>%select(Pos,everything(),U=T)%>%write_tsv('ext_data/tops_yamashita_etal_2008/top_pwm.txt')
#  
pwmfolder = '../ext_data/mouse_cisrbp/pwms_all_motifs/'
pwms = list(filter(lambda f: '.meme' in f, list(os.walk(pwmfolder))[0][2]))

rule motscans:
  input: fasta='motseqs/{reg}/{set}/{set}.fa',pwmfolder='../ext_data/mouse_cisrbp/pwms_all_motifs'
  output: touch('motscans/{reg}/{set}/.done')
  run:
    backgroundfasta = input.fasta.replace('.fa','_background.fa')
    pwmfolder = input['pwmfolder']+'/'
    pwmfiles = [pwmfolder + f for f in list(os.walk(pwmfolder))[0][2] if '.meme' in f]
    pwmfiles = [ f for f in pwmfiles if 'M287' in f]
    outputdir = os.path.dirname(output[0])
    shell(r"""
        source activate meme
        for pwmfile in {pwmfiles}; do
          echo $pwmfile
          pwmbase=$(basename ${{pwmfile%.txt.meme}})
          rm -rf {outputdir}/$pwmbase
          mast -ev 1 --o {outputdir}/$pwmbase $pwmfile {input.fasta} 
          mast -ev 1 --o {outputdir}/${{pwmbase}}_background $pwmfile {backgroundfasta} 
        done
      """)

rule gquad_enrich:
  input:
    seqfile = 'motseqs/{reg}/{set}/{set}.fa',
    seqfileback = 'motseqs/{reg}/nontransregged.fa',
  output: touch('gquad_enrich/{reg}/{set}/.done')
  threads: 8
  shell: r'''
    Rscript ../exploration/quads.R \
    {input.seqfile} \
    {input.seqfileback} \
    $(dirname {output})
  '''

#cat my_gencode.vM12.annotation.gtf | grep transcript  |  awk '{if($7=="+"){$5=$4}else{$4=$5};print $1,$4,$5,".",".",$7}' | awk -vOFS="\t" '{$2 = $2 - 100; $3 = $3 + 100;print $0}' | bedtools getfasta -bed - -fi my_GRCm38.p5.genome.chr_scaff.fa -fo /dev/stdout -s > promotorseqs.fa
# samtools view -H star/data/E13_ribo_1/E13_ribo_1.bam | grep "SQ" | sed -r 's/^@SQ\s+SN://' | sed -r "s/\s+LN:/\t/" > my_GRCm38.p5.genome.chr_scaff.chromsizes
#cat my_gencode.vM12.annotation.gtf | awk '$3=="transcript"'  | awk -vOFS="\t" '{if($7=="+"){$5=$4}else{$4=$5};print $1,$4,$5,$12,".",$7}'|sed 's/[";]//g' | bedtools slop -b 100 -i - -g my_GRCm38.p5.genome.chr_scaff.chromsizes | bedtools getfasta -bed - -fi my_GRCm38.p5.genome.chr_scaff.fa -fo /dev/stdout -s  -name > promotorseqs.fa 

#now scan with our top motif
#cat fimo_out/fimo.gff | cut -f 1  | sort | uniq | wc -l ; grep -c -e '>' ../ext_data/tops_yamashita_etal_2008/supp2seqs.fa
chrsizes = REF.replace(".fa",".chromsizes")


rule fpTOP_scan:
  input: toppwm='../ext_data/tops_yamashita_etal_2008/top_pwm.txt.meme',
  output: 'fpTOP_scan/fimo.gff'
  shell: r"""
  
    mkdir -p motseqs

    cat {GTF} \
      | awk '$3=="transcript"'  \
      | awk -vOFS="\t" '{{if($7=="+"){{$5=$4}}else{{$4=$5}};print $1,$4,$5,$12,".",$7}}'\
      | sed 's/[";]//g' \
      | bedtools slop -b 11 -i - -g {chrsizes}\
      | bedtools getfasta -bed - -fi {REF} -fo /dev/stdout -s  -name \
      > motseqs/promotorseqs.fa

    fimo --oc $(dirname {output} ) {input.toppwm} motseqs/promotorseqs.fa 

  """


# rule set_enrich:
#   input:

#     genesetfile = 'gene_set_enrichment/allgenesets.gmt',
#   output: touch('gquad_enrich/{reg}/{set}/.done')
#   params:
#   threads: 8
#   shell: r'''

#     R -e 'rmarkdown::render("{input.ribqc_report}",params = list(input_list = "{indata}",input_list_names = "{sinputs}", output_fig_path = "{figpath}"),output_file = "{outpath}")'
#     Rscript ../exploration/quads.R \
#     {input.seqfile} \
#     {input.seqfileback} \
#     $(dirname {output})
#   '''




##########Mappability
for kmer in MAPPABILITY_LENGTHS:
  kmer = str(kmer)
  ASSAY_DICT['mappability_'+kmer] = 'ribo'
  READ_PATTERN_DICT['mappability_'+kmer] = '-f *.fastq.gz'
  SAMPLEFASTQDICT['mappability_'+kmer] = ['input/mappability_'+kmer+'/mappability_'+kmer+'.fastq.gz']

mappability_refbase = 'mappability/index/mref'
mappability_ref = mappability_refbase +'.fa'
mindex = ['mappability/index/mref.'+str(i)+'.bt2' for i in range(1,5)]
contaminants = filter_index.replace('.done','.fa')

# rule create_mappability_ref:
#   input: REF,filter_index
#   output: touch('mappability/index/.done')
#   shell: r"""
#     cat {REF} {contaminants} > {mappability_refbase}.fa
#     bowtie2-build {mappability_refbase}.fa {mappability_refbase}
#   """

rule create_mappability_reads:
  input: trsizes,REF
  output: 'trim_reads/mappability_{kmer}/mappability_{kmer}.fastq.gz'
  threads: 8
  shell: r"""
  #do this for entire genome
  set -x
  #these one liners, in order, cat the chromosome sizesinto awk to create bed files spanning the chromosomes, 
  #create fastas from a bed file, stick together every second line, generate the tiles of kmer size, modify the names of hte
  #tiles, and then finally turn them into fastq format
 
  cat {trsizes} \
    | awk '{{print $1"\t"1"\t"$2}}' \
    | bedtools getfasta -s -fi {RNAFASTA} -bed -  \
    | sed '$!N;s/\n/ /' \
    | perl -lane '$i=0;while($i<=(length($F[1])-{wildcards.kmer})){{print $F[0] , "$i\n", substr $F[1],$i,{wildcards.kmer} ; $i = $i +1}}' \
    | perl -lan -F'[\:\-|\)|\(]' -e 'if( /^>/){{print $F[0],"_",$F[1]+$F[4],"_",$F[1]+$F[4]+{wildcards.kmer}}}else{{print @F}}' \
    | perl -lanpe 's/>/@/ ; s/^([^@]+)/\1\n+/; if($1){{$a="I" x length($1); s/\+/+\n$a/}}' \
    | gzip > {output}
    """
# rule create_mappability_bam:
#   input: 'mappability/mappability_reads.{kmer}.fastq.gz'
#   output: 'mappability/mappability_{kmer}.bam'
#   shell: r"""
#   source activate tophat
#   set -x
#   mkdir -p mappability/mapped_{wildcards.kmer}
#    tophat2 \
#           -p {threads} \
#           -z0 \
#           -g 20 \
#           --output-dir mappability/mapped_{wildcards.kmer} \
#           --library-type fr-unstranded \
#           --no-coverage-search \
#           -no_novel-juncs -N 1 \
#           {mappability_refbase} \
#           mappability/mappability_reads.{wildcards.kmer}.fastq.gz

    
#     samtools sort -@ {threads} mappability/mapped_{wildcards.kmer}/accepted_hits.bam -o {output}
#   """

# rule create_mappability_bam:
#   input: 'mappability/mappability_reads.{kmer}.fastq.gz'
#   output: 'mappability/mappability_{kmer}.bam'
#   params: 
#     ref=REF,
#     maxmmap=20
#   resources:
#     mem=int(50*1e9)
#   shell: r"""
#   #conda create -n bbmap bbmap
#   source activate bbmap
#   set -x
#   bbmap.sh ref={params.ref} in={input} out={output} ambig=all nodisk vslow perfectmode maxsites={params.maxmmap}    
#   """

rule createmappability_bedgraph:
  # input: 'mappability/mappability_{kmer}.bam'
  input: 'star/data/mappability_{kmer}/.done',trsizes
  output: 'mappability/mappability_{kmer}.bedgraph'
  conda: '../envs/bedgraphtobigwig'
  params:
    bam = lambda wc: 'star/data/mappability_'+str(wc['kmer'])+'/mappability_'+str(wc['kmer'])+'.bam'
  threads: 8
  shell: r"""
  trap "set -x; rm -rf ${{MY_TMP_DIR}}" EXIT KILL TERM INT HUP

  MY_TMP_DIR=$(mktemp -d)
  mkdir -p $MY_TMP_DIR
  MY_TMP_DIR='mappatest'
 
  set -x

  samtools view -h {params.bam}  | sed -e '/^@PG/Q' \
   | cut -f 2,3 | sed 's/[SL]N://g' | sed 's/ /_/g' \
   | tail -n+2 >${{MY_TMP_DIR}}/chrsizes

  samtools view {params.bam} \
    | grep -ve 'NH:i:1' \
    | awk -vFS="_" -vOFS="\t" '{{print $1,$2-1,$2}}' \
    | uniq \
    | bedtools sort -i /dev/stdin -faidx {trsizes} \
    | bedtools merge  -prec 1 \
    | awk -vOFS="\t" '{{print $0,1}}' > {output[0]}

    
#  bedGraphToBigWig ${{MY_TMP_DIR}}/nomap.bg {trsizes} {output[0]}

"""

#echo -e "chr1\t10000000\t10000030\tchr1_10000000_10000030" | bedtools getfasta -s -fi my_GRCm38.p5.genome.chr_scaff.fa -bed -  -name | sed '$!N;s/\n/_/' | tr -d '>' | perl -F'_' -lane '$i=0;while($i<=(length($F[3])-10)){print $F[0] ,"_",$F[1]+$i," ",substr $F[3],$i,10 ; $i = $i +1}'

#echo -e "chr1\t10000000\t10000030\tchr1_10000000_10000030" | bedtools getfasta -s -fi my_GRCm38.p5.genome.chr_scaff.fa -bed -  -name | sed '$!N;s/\n/_/' | tr -d '>' | perl -F'_' -lane '$i=0;while($i<=(length($F[3])-10)){print $F[0] ,"_",$F[1]+$i," ",substr $F[3],$i,10 ; $i = $i +1}'

#samtools view mappability/mappa_27.bam | grep -Fve 'NH:i:1' | cut -d '_' -f 1,2 | gzip > mappability_27.txt.gz

#-transcriptome_only 
           
#samtools view mappability/mappability_27.bam  | grep -ve 'NH:i:1'     | awk -vFS="_" -vOFS="\t" '{{print $1,$2-1,$2}}' | uniq     | bedtools sort -i /dev/stdin     | bedtools merge  -prec 1 

#rule run_template:
#  input: templatefinput
#  output: touch('run_template/.done')
#  shell: r"""template {input} $(dirname {output})"""